
الحمد لله
اشتغل



لينكس 24
بايثون 3ز12
uv



uv pip install leann

https://pypi.org/project/leann/0.3.5/
https://github.com/yichuan-w/LEANN


بدل
Qwen/Qwen3-0.6B
استخدم
Qwen/Qwen2.5-0.5B-Instruct




















إليك الأكواد الثلاثة الناجحة التي تم استخلاصها من جلسة الطرفية (Terminal) الخاصة بك، وهي جاهزة للنسخ والتشغيل مباشرة دون المخرجات (Logs):
الكود الأول: (إصلاح وتحميل الشات فقط)
هذا الكود استُخدم عندما كان الفهرس (Index) مبنياً بالفعل، وقمتَ بإعادة تعريف المسارات وتشغيل الشات بالموديل الصحيح.
code
Python
from leann import LeannChat
from pathlib import Path

# 1. تعريف المسار واسم الموديل
INDEX_PATH = str(Path("./").resolve() / "demo.leann")
model_name = "Qwen/Qwen2.5-0.5B-Instruct"

# 2. تشغيل الشات
chat = LeannChat(INDEX_PATH, llm_config={
    "type": "hf", 
    "model": model_name
})

# 3. طرح السؤال وطباعة النتيجة
response = chat.ask("How much storage does LEANN save?", top_k=1)
print(response)
الكود الثاني: (مثال لغات البرمجة مع تفعيل السجلات Log Level)
هذا الكود يبني قاعدة بيانات عن لغات البرمجة (C#, Python)، يقوم بالبحث، ثم يقارن بينهما باستخدام الذكاء الاصطناعي.
code
Python
import os
from pathlib import Path
from leann.api import LeannBuilder, LeannSearcher, LeannChat

# تفعيل السجلات لرؤية التفاصيل
os.environ["LEANN_LOG_LEVEL"] = "INFO" 

INDEX_DIR = Path("./").resolve()
INDEX_PATH = str(INDEX_DIR / "demo.leann")

# 1. بناء الفهرس (Building Index)
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("C# is a powerful programming language and it is good at game development")
builder.add_text("Python is a powerful programming language and it is good at machine learning tasks")
builder.add_text("Machine learning transforms industries")
builder.add_text("Neural networks process complex data")
builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
builder.build_index(INDEX_PATH)

# 2. البحث (Searching)
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("programming languages", top_k=2)
print(results)

# 3. المحادثة (Chatting)
llm_config = {
    "type": "hf",
    "model": "Qwen/Qwen2.5-0.5B-Instruct", # الموديل المصحح
}

chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)

response = chat.ask(
    "Compare the two retrieved programming languages and tell me their advantages.",
    top_k=2,
    llm_kwargs={"max_tokens": 128},
)
print(response)
الكود الثالث: (المثال الأساسي الكامل - Demo)
هذا هو الكود القياسي للمكتبة الذي يقوم بالعملية كاملة (بناء، بحث، وسؤال) في سكريبت واحد.
code
Python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path

INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# 1. Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
builder.build_index(INDEX_PATH)

# 2. Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)
# (اختياري) طباعة نتائج البحث
# print(results) 

# 3. Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen2.5-0.5B-Instruct"})
response = chat.ask("How much storage does LEANN save?", top_k=1)

print(response)
49.4s
Use Arrow Up and Arrow Down to select a turn, Enter to jump to it, and Escape to return to the chat.
Start typing a prompt



















m@m-HP-Z440-Workstation:~/1$ uv venv --python 3.12 venv && source venv/bin/activate
Using CPython 3.12.3 interpreter at: /usr/bin/python3.12
Creating virtual environment at: venv
Activate with: source venv/bin/activate
(venv) m@m-HP-Z440-Workstation:~/1$ uv pip install leann
Using Python 3.12.3 environment at: venv
Resolved 140 packages in 3.71s
      Built gitignore-parser==0.1.13
⠧ Preparing packages... (51/52)                                              Prepared 52 packages in 21m 52s
Installed 140 packages in 301ms
 + accelerate==1.12.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + aiosqlite==0.21.0
 + annotated-types==0.7.0
 + anyio==4.11.0
 + attrs==25.4.0
 + banks==2.2.0
 + beautifulsoup4==4.14.2
 + bleach==6.3.0
 + certifi==2025.11.12
 + cffi==2.0.0
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorama==0.4.6
 + cryptography==46.0.3
 + dataclasses-json==0.6.7
 + defusedxml==0.7.1
 + deprecated==1.3.1
 + dirtyjson==1.0.8
 + distro==1.9.0
 + fastjsonschema==2.21.2
 + filelock==3.20.0
 + filetype==1.2.0
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitignore-parser==0.1.13
 + greenlet==3.2.4
 + griffe==1.15.0
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + idna==3.11
 + jinja2==3.1.6
 + jiter==0.12.0
 + joblib==1.5.2
 + jsonschema==4.25.1
 + jsonschema-specifications==2025.9.1
 + jupyter-client==8.6.3
 + jupyter-core==5.9.1
 + jupyterlab-pygments==0.3.0
 + leann==0.3.5
 + leann-backend-diskann==0.3.5
 + leann-backend-hnsw==0.3.5
 + leann-core==0.3.5
 + llama-index-core==0.14.8
 + llama-index-embeddings-huggingface==0.6.1
 + llama-index-instrumentation==0.4.2
 + llama-index-readers-file==0.5.5
 + llama-index-workflows==2.11.4
 + markupsafe==3.0.3
 + marshmallow==3.26.1
 + mistune==3.1.4
 + mpmath==1.3.0
 + msgpack==1.1.2
 + multidict==6.7.0
 + mypy-extensions==1.1.0
 + nbclient==0.10.2
 + nbconvert==7.16.6
 + nbformat==5.10.4
 + nest-asyncio==1.6.0
 + networkx==3.5
 + nltk==3.9.2
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + openai==2.8.1
 + packaging==25.0
 + pandas==2.2.3
 + pandocfilters==1.5.1
 + pdfminer-six==20251107
 + pdfplumber==0.11.8
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + psutil==7.1.3
 + pycparser==2.23
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pygments==2.19.2
 + pymupdf==1.26.6
 + pypdf==6.4.0
 + pypdf2==3.0.1
 + pypdfium2==5.1.0
 + python-dateutil==2.9.0.post0
 + python-dotenv==1.2.1
 + pytz==2025.2
 + pyyaml==6.0.3
 + pyzmq==27.1.0
 + referencing==0.37.0
 + regex==2025.11.3
 + requests==2.32.5
 + rpds-py==0.29.0
 + safetensors==0.7.0
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + sentence-transformers==5.1.2
 + setuptools==80.9.0
 + six==1.17.0
 + sniffio==1.3.1
 + soupsieve==2.8
 + sqlalchemy==2.0.44
 + striprtf==0.0.26
 + sympy==1.14.0
 + tenacity==9.1.2
 + threadpoolctl==3.6.0
 + tiktoken==0.12.0
 + tinycss2==1.4.0
 + tokenizers==0.20.3
 + torch==2.9.1
 + tornado==6.5.2
 + tqdm==4.67.1
 + traitlets==5.14.3
 + transformers==4.45.2
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspect==0.9.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + webencodings==0.5.1
 + wrapt==2.0.1
 + yarl==1.22.0
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
WARNING - leann.embedding_compute - Local loading failed (We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like facebook/contriever is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.), trying network download...
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
config.json: 100%|█████████████████████████████████████████████████████████████████████| 619/619 [00:00<00:00, 2.43MB/s]
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████| 438M/438M [01:57<00:00, 3.73MB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████| 321/321 [00:00<00:00, 2.37MB/s]
vocab.txt: 232kB [00:00, 6.29MB/s]
tokenizer.json: 466kB [00:00, 8.02MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 563kB/s]
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14266.34chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.36it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.19s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.37s]   Read levels (2)
[0.55s]   Probing for compact storage flag...
[0.55s]   Found compact flag: False
[0.55s]   Compact flag is False, reading original format...
[0.55s]   Probing for potential extra byte before non-compact offsets...
[0.55s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.55s]   Read offsets (3)
[0.73s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.73s]   Read neighbors (128)
[0.91s]   Read scalar params (ep=1, max_lvl=0)
[0.91s] Checking for storage data...
[0.91s]   Found storage fourcc: 49467849.
[0.91s] Converting to CSR format...
[0.92s]   Conversion loop finished.                        
[0.92s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.92s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.10s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.29s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": "Qwen/Qwen2.5-0.5B-Instruct"
... })
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
tokenizer_config.json: 7.30kB [00:00, 15.4MB/s]
vocab.json: 2.78MB [00:00, 8.45MB/s]
merges.txt: 1.67MB [00:00, 8.92MB/s]
tokenizer.json: 7.03MB [00:00, 13.9MB/s]
config.json: 100%|█████████████████████████████████████████████████████████████████████| 659/659 [00:00<00:00, 5.21MB/s]
model.safetensors: 100%|█████████████████████████████████████████████████████████████| 988M/988M [04:01<00:00, 4.10MB/s]
ERROR - leann.chat - Model loading timed out for Qwen/Qwen2.5-0.5B-Instruct
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3604, in from_pretrained
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1168, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1720, in _download_to_tmp_and_move
    xet_get(
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 626, in xet_get
    download_files(
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 602, in timeout_handler
    raise TimeoutError("Model download/loading timed out")
TimeoutError: Model download/loading timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 628, in __init__
    raise RuntimeError(
RuntimeError: Model loading timed out for Qwen/Qwen2.5-0.5B-Instruct. Please check your internet connection or try a smaller model.
>>> 
>>> # 2. Now you can ask your question
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> print(response)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10118.95chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.48it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.23s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.45s]   Read levels (2)
[0.65s]   Probing for compact storage flag...
[0.65s]   Found compact flag: False
[0.65s]   Compact flag is False, reading original format...
[0.65s]   Probing for potential extra byte before non-compact offsets...
[0.65s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.65s]   Read offsets (3)
[0.83s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.84s]   Read neighbors (128)
[1.02s]   Read scalar params (ep=1, max_lvl=0)
[1.02s] Checking for storage data...
[1.02s]   Found storage fourcc: 49467849.
[1.02s] Converting to CSR format...
[1.02s]   Conversion loop finished.                        
[1.02s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[1.02s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.20s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.39s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO" 
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> 
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13971.70chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.62it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.24s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.47s]   Read levels (5)
[0.66s]   Probing for compact storage flag...
[0.66s]   Found compact flag: False
[0.66s]   Compact flag is False, reading original format...
[0.66s]   Probing for potential extra byte before non-compact offsets...
[0.66s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.66s]   Read offsets (6)
[0.85s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.85s]   Read neighbors (320)
[1.04s]   Read scalar params (ep=4, max_lvl=0)
[1.04s] Checking for storage data...
[1.04s]   Found storage fourcc: 49467849.
[1.04s] Converting to CSR format...
[1.04s]   Conversion loop finished.                        
[1.04s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[1.04s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.24s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.43s] Conversion complete.
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.

>>> results = searcher.search("programming languages", top_k=2)
2025-11-23 22:29:54,831 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:29:54,831 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:29:54,833 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:29:54,834 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:29:54,834 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:29:54,835 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:29:54,835 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:29:54,835 - INFO - HNSW ZMQ REP server listening on port 5557
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.37735939025878906 seconds
2025-11-23 22:29:59,934 - INFO - ⏱️  Text embedding E2E time: 5.098317s
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:29:59,936 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.016899585723876953 seconds
2025-11-23 22:29:59,953 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:29:59,954 - INFO - ⏱️  ZMQ E2E time: 0.019576s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02208852767944336 seconds
2025-11-23 22:29:59,977 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:29:59,977 - INFO - ⏱️  Distance calculation E2E time: 0.023890s
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
2025-11-23 22:30:38,307 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:30:38,308 - INFO - Initiating graceful shutdown...
2025-11-23 22:30:38,308 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:30:39,025 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:30:39,026 - INFO - ZMQ resources cleaned up
2025-11-23 22:30:39,262 - INFO - Additional resources cleaned up
2025-11-23 22:30:39,262 - INFO - Graceful shutdown completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> 
>>> model_name = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> print("جاري تحميل الموديل إلى الذاكرة المؤقتة (Cache)...")
جاري تحميل الموديل إلى الذاكرة المؤقتة (Cache)...
>>> # هذا الأمر سيقوم بتنزيل الملفات ولن يفصل بسبب الوقت
>>> AutoModelForCausalLM.from_pretrained(model_name)
model.safetensors: 100%|█████████████████████████████████████████████████████████████| 988M/988M [04:01<00:00, 4.09MB/s]
generation_config.json: 100%|██████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 1.01MB/s]
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 896)
    (layers): ModuleList(
      (0-23): 24 x Qwen2DecoderLayer(
        (self_attn): Qwen2SdpaAttention(
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (k_proj): Linear(in_features=896, out_features=128, bias=True)
          (v_proj): Linear(in_features=896, out_features=128, bias=True)
          (o_proj): Linear(in_features=896, out_features=896, bias=False)
          (rotary_emb): Qwen2RotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
          (up_proj): Linear(in_features=896, out_features=4864, bias=False)
          (down_proj): Linear(in_features=4864, out_features=896, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((896,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=896, out_features=151936, bias=False)
)
>>> AutoTokenizer.from_pretrained(model_name)
Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
>>> print("تم التحميل بنجاح!")
تم التحميل بنجاح!
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": model_name
... })
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'LeannChat' is not defined
>>> 
>>> # 3. الآن يمكنك طرح السؤال
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> print(response)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'INDEX_PATH' is not defined
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6423.13chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 61.24it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.33s]   Read levels (2)
[0.49s]   Probing for compact storage flag...
[0.50s]   Found compact flag: False
[0.50s]   Compact flag is False, reading original format...
[0.50s]   Probing for potential extra byte before non-compact offsets...
[0.50s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.50s]   Read offsets (3)
[0.66s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.66s]   Read neighbors (128)
[0.82s]   Read scalar params (ep=1, max_lvl=0)
[0.82s] Checking for storage data...
[0.82s]   Found storage fourcc: 49467849.
[0.82s] Converting to CSR format...
[0.82s]   Conversion loop finished.                        
[0.82s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.82s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[0.99s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.16s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannChat
>>> from pathlib import Path
>>> 
>>> # 1. تعريف المسار واسم الموديل مجدداً
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> model_name = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> # 2. تشغيل الشات (الآن سيعمل لأن الموديل تم تحميله والتعريفات موجودة)
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": model_name
... })
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> 
>>> # 3. طرح السؤال
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Based on the information provided in the context, LEANN saves 97% of the storage capacity compared to traditional vector databases.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO" 
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
INFO - leann.embedding_compute - Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO - leann.embedding_compute - Using device: cuda
INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: facebook/contriever
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO - leann.embedding_compute - Model loaded successfully! (local + optimized)
INFO - leann.embedding_compute - Applied FP16 precision: facebook/contriever
INFO - leann.embedding_compute - Applied torch.compile optimization: facebook/contriever
INFO - leann.embedding_compute - Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO - leann.embedding_compute - Generated 1 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.38034653663635254 seconds
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 23994.87chunk/s]
INFO - leann.embedding_compute - Computing embeddings for 5 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Using cached optimized model: facebook/contriever
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 50.82it/s]
INFO - leann.embedding_compute - Generated 5 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.02299022674560547 seconds
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
INFO - leann_backend_hnsw.hnsw_backend - INFO: Converting HNSW index to CSR-pruned format...
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.18s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.36s]   Read levels (5)
[0.53s]   Probing for compact storage flag...
[0.53s]   Found compact flag: False
[0.53s]   Compact flag is False, reading original format...
[0.53s]   Probing for potential extra byte before non-compact offsets...
[0.53s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.53s]   Read offsets (6)
[0.71s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.71s]   Read neighbors (320)
[0.88s]   Read scalar params (ep=4, max_lvl=0)
[0.88s] Checking for storage data...
[0.88s]   Found storage fourcc: 49467849.
[0.88s] Converting to CSR format...
[0.88s]   Conversion loop finished.                        
[0.88s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.88s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.06s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.24s] Conversion complete.
INFO - leann_backend_hnsw.hnsw_backend - ✅ CSR conversion successful.
INFO - leann_backend_hnsw.hnsw_backend - INFO: Replaced original index with CSR-pruned version at '/home/m/1/demo.index'
>>> 
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("programming languages", top_k=2)
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'programming languages'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5557...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 12682

2025-11-23 22:42:22,477 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:42:22,477 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:42:22,479 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:42:22,480 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:42:22,480 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:42:22,481 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:42:22,481 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:42:22,481 - INFO - HNSW ZMQ REP server listening on port 5557
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.0042552947998047 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.3753476142883301 seconds
2025-11-23 22:42:27,604 - INFO - ⏱️  Text embedding E2E time: 5.122752s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.7245402336120605 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:42:27,607 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.01667332649230957 seconds
2025-11-23 22:42:27,625 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:42:27,625 - INFO - ⏱️  ZMQ E2E time: 0.020548s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02157139778137207 seconds
2025-11-23 22:42:27,648 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:42:27,648 - INFO - ⏱️  Distance calculation E2E time: 0.023301s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04257988929748535 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.04282116889953613 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 0.9876 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.8924 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
INFO - leann.chat - Attempting to create LLM of type='hf' with model='Qwen/Qwen3-0.6B'
INFO - leann.chat - Initializing HFChat with model='Qwen/Qwen3-0.6B'
INFO - leann.chat - CUDA is available. Using GPU.
INFO - leann.chat - Loading tokenizer for Qwen/Qwen3-0.6B...
INFO - leann.chat - Loading model Qwen/Qwen3-0.6B...
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> exit()
INFO - leann.embedding_server_manager - Terminating server process (PID: 12682) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:43:48,768 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:43:48,769 - INFO - Initiating graceful shutdown...
2025-11-23 22:43:48,770 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:43:49,740 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:43:49,741 - INFO - ZMQ resources cleaned up
2025-11-23 22:43:49,961 - INFO - Additional resources cleaned up
2025-11-23 22:43:49,961 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 12682 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 12682 cleanup completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO"
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
INFO - leann.embedding_compute - Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO - leann.embedding_compute - Using device: cuda
INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: facebook/contriever
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO - leann.embedding_compute - Model loaded successfully! (local + optimized)
INFO - leann.embedding_compute - Applied FP16 precision: facebook/contriever
INFO - leann.embedding_compute - Applied torch.compile optimization: facebook/contriever
INFO - leann.embedding_compute - Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO - leann.embedding_compute - Generated 1 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.4149184226989746 seconds
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 26546.23chunk/s]
INFO - leann.embedding_compute - Computing embeddings for 5 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Using cached optimized model: facebook/contriever
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.92it/s]
INFO - leann.embedding_compute - Generated 5 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.02059483528137207 seconds
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
INFO - leann_backend_hnsw.hnsw_backend - INFO: Converting HNSW index to CSR-pruned format...
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.35s]   Read levels (5)
[0.52s]   Probing for compact storage flag...
[0.52s]   Found compact flag: False
[0.52s]   Compact flag is False, reading original format...
[0.52s]   Probing for potential extra byte before non-compact offsets...
[0.52s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.52s]   Read offsets (6)
[0.70s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.70s]   Read neighbors (320)
[0.87s]   Read scalar params (ep=4, max_lvl=0)
[0.87s] Checking for storage data...
[0.87s]   Found storage fourcc: 49467849.
[0.87s] Converting to CSR format...
[0.87s]   Conversion loop finished.                        
[0.87s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.87s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.05s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.23s] Conversion complete.
INFO - leann_backend_hnsw.hnsw_backend - ✅ CSR conversion successful.
INFO - leann_backend_hnsw.hnsw_backend - INFO: Replaced original index with CSR-pruned version at '/home/m/1/demo.index'
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("programming languages", top_k=2)
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'programming languages'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5557...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 12908
2025-11-23 22:45:50,958 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:45:50,958 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:45:50,960 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:45:50,961 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:45:50,961 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:45:50,961 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:45:50,961 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:45:50,962 - INFO - HNSW ZMQ REP server listening on port 5557
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.004446506500244 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.39574456214904785 seconds
2025-11-23 22:45:56,207 - INFO - ⏱️  Text embedding E2E time: 5.244366s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.863163709640503 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:45:56,210 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.018288373947143555 seconds
2025-11-23 22:45:56,229 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:45:56,229 - INFO - ⏱️  ZMQ E2E time: 0.022117s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.019530057907104492 seconds
2025-11-23 22:45:56,250 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:45:56,250 - INFO - ⏱️  Distance calculation E2E time: 0.021174s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04223132133483887 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.042572021484375 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 0.9876 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.8924 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen2.5-0.5B-Instruct",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
INFO - leann.chat - Attempting to create LLM of type='hf' with model='Qwen/Qwen2.5-0.5B-Instruct'
INFO - leann.chat - Initializing HFChat with model='Qwen/Qwen2.5-0.5B-Instruct'
INFO - leann.chat - CUDA is available. Using GPU.
INFO - leann.chat - Loading tokenizer for Qwen/Qwen2.5-0.5B-Instruct...
INFO - leann.chat - Loading model Qwen/Qwen2.5-0.5B-Instruct...
INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO - leann.chat - Successfully loaded Qwen/Qwen2.5-0.5B-Instruct
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'Compare the two retrieved programming languages and tell me their advantages.'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5558...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 13030
2025-11-23 22:46:34,609 - INFO - Starting HNSW server on port 5558 with model facebook/contriever
2025-11-23 22:46:34,609 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:46:34,611 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:46:34,612 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:46:34,612 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:46:34,612 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:46:34,612 - INFO - Started HNSW ZMQ server thread on port 5558
2025-11-23 22:46:34,613 - INFO - HNSW ZMQ REP server listening on port 5558
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.003239870071411 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.36855506896972656 seconds
2025-11-23 22:46:39,595 - INFO - ⏱️  Text embedding E2E time: 4.980902s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.586749315261841 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:46:39,597 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.017423391342163086 seconds
2025-11-23 22:46:39,615 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:46:39,615 - INFO - ⏱️  ZMQ E2E time: 0.020210s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02080988883972168 seconds
2025-11-23 22:46:39,637 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:46:39,637 - INFO - ⏱️  Distance calculation E2E time: 0.022386s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04146623611450195 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.04176211357116699 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 1.0102 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.9646 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
INFO - leann.api -   Search time: 6.6329734325408936 seconds
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.010      | 0          | C# is a powerful programming language and it is good at game |                                                                                 
0.965      | 1          | Python is a powerful programming language and it is good at  |                                                                                 
kwargs in HF:  {'max_tokens': 128}
INFO - leann.chat - Generating with HuggingFace model, config: {'max_new_tokens': 128, 'temperature': 0.7, 'top_p': 0.9, 'do_sample': True, 'pad_token_id': 151645, 'eos_token_id': 151645}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
INFO - leann.api -   Ask time: 5.142719268798828 seconds
>>> response
'Based on the information provided in the context, C# and Python both offer significant advantages for game development and machine learning tasks respectively.\n\n**C# (Microsoft)**\n- **Game Development**: C# is highly favored in game development due to its strong support for Unity, Unreal Engine, and other popular game engines. It provides robust libraries and frameworks specifically designed for game development.\n- **Machine Learning Tasks**: Although C# has its limitations when it comes to machine learning, particularly in terms of specific algorithms or frameworks like TensorFlow, it still offers flexibility and ease of use compared to many other languages.\n- **Compatibility**: While not as widely used'
>>> exit()
INFO - leann.embedding_server_manager - Terminating server process (PID: 13030) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:47:34,193 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:47:34,193 - INFO - Initiating graceful shutdown...
2025-11-23 22:47:34,194 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:47:34,701 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:47:34,702 - INFO - ZMQ resources cleaned up
2025-11-23 22:47:34,937 - INFO - Additional resources cleaned up
2025-11-23 22:47:34,937 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 13030 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 13030 cleanup completed
INFO - leann.embedding_server_manager - Terminating server process (PID: 12908) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:47:36,162 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:47:36,162 - INFO - Initiating graceful shutdown...
2025-11-23 22:47:36,162 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:47:36,365 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:47:36,366 - INFO - ZMQ resources cleaned up
2025-11-23 22:47:36,591 - INFO - Additional resources cleaned up
2025-11-23 22:47:36,591 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 12908 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 12908 cleanup completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10908.46chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.12it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.34s]   Read levels (2)
[0.50s]   Probing for compact storage flag...
[0.50s]   Found compact flag: False
[0.50s]   Compact flag is False, reading original format...
[0.50s]   Probing for potential extra byte before non-compact offsets...
[0.50s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.50s]   Read offsets (3)
[0.67s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.67s]   Read neighbors (128)
[0.84s]   Read scalar params (ep=1, max_lvl=0)
[0.84s] Checking for storage data...
[0.84s]   Found storage fourcc: 49467849.
[0.84s] Converting to CSR format...
[0.84s]   Conversion loop finished.                        
[0.84s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.84s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.00s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.18s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen2.5-0.5B-Instruct"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Based on the information provided in the context, LEANN saves 97% of the available storage space compared to traditional vector databases. Therefore, LEANN typically offers up to 97% more storage capacity than a vector database.
>>> exit()

(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 



















m@m-HP-Z440-Workstation:~/1$ uv venv --python 3.12 venv && source venv/bin/activate
Using CPython 3.12.3 interpreter at: /usr/bin/python3.12
Creating virtual environment at: venv
Activate with: source venv/bin/activate
(venv) m@m-HP-Z440-Workstation:~/1$ uv pip install leann
Using Python 3.12.3 environment at: venv
Resolved 140 packages in 3.71s
      Built gitignore-parser==0.1.13
⠧ Preparing packages... (51/52)Prepared 52 packages in 21m 52s
Installed 140 packages in 301ms
 + accelerate==1.12.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + aiosqlite==0.21.0
 + annotated-types==0.7.0
 + anyio==4.11.0
 + attrs==25.4.0
 + banks==2.2.0
 + beautifulsoup4==4.14.2
 + bleach==6.3.0
 + certifi==2025.11.12
 + cffi==2.0.0
 + charset-normalizer==3.4.4
 + click==8.3.1
 + colorama==0.4.6
 + cryptography==46.0.3
 + dataclasses-json==0.6.7
 + defusedxml==0.7.1
 + deprecated==1.3.1
 + dirtyjson==1.0.8
 + distro==1.9.0
 + fastjsonschema==2.21.2
 + filelock==3.20.0
 + filetype==1.2.0
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitignore-parser==0.1.13
 + greenlet==3.2.4
 + griffe==1.15.0
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + idna==3.11
 + jinja2==3.1.6
 + jiter==0.12.0
 + joblib==1.5.2
 + jsonschema==4.25.1
 + jsonschema-specifications==2025.9.1
 + jupyter-client==8.6.3
 + jupyter-core==5.9.1
 + jupyterlab-pygments==0.3.0
 + leann==0.3.5
 + leann-backend-diskann==0.3.5
 + leann-backend-hnsw==0.3.5
 + leann-core==0.3.5
 + llama-index-core==0.14.8
 + llama-index-embeddings-huggingface==0.6.1
 + llama-index-instrumentation==0.4.2
 + llama-index-readers-file==0.5.5
 + llama-index-workflows==2.11.4
 + markupsafe==3.0.3
 + marshmallow==3.26.1
 + mistune==3.1.4
 + mpmath==1.3.0
 + msgpack==1.1.2
 + multidict==6.7.0
 + mypy-extensions==1.1.0
 + nbclient==0.10.2
 + nbconvert==7.16.6
 + nbformat==5.10.4
 + nest-asyncio==1.6.0
 + networkx==3.5
 + nltk==3.9.2
 + numpy==2.3.5
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + openai==2.8.1
 + packaging==25.0
 + pandas==2.2.3
 + pandocfilters==1.5.1
 + pdfminer-six==20251107
 + pdfplumber==0.11.8
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.1
 + psutil==7.1.3
 + pycparser==2.23
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pygments==2.19.2
 + pymupdf==1.26.6
 + pypdf==6.4.0
 + pypdf2==3.0.1
 + pypdfium2==5.1.0
 + python-dateutil==2.9.0.post0
 + python-dotenv==1.2.1
 + pytz==2025.2
 + pyyaml==6.0.3
 + pyzmq==27.1.0
 + referencing==0.37.0
 + regex==2025.11.3
 + requests==2.32.5
 + rpds-py==0.29.0
 + safetensors==0.7.0
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + sentence-transformers==5.1.2
 + setuptools==80.9.0
 + six==1.17.0
 + sniffio==1.3.1
 + soupsieve==2.8
 + sqlalchemy==2.0.44
 + striprtf==0.0.26
 + sympy==1.14.0
 + tenacity==9.1.2
 + threadpoolctl==3.6.0
 + tiktoken==0.12.0
 + tinycss2==1.4.0
 + tokenizers==0.20.3
 + torch==2.9.1
 + tornado==6.5.2
 + tqdm==4.67.1
 + traitlets==5.14.3
 + transformers==4.45.2
 + triton==3.5.1
 + typing-extensions==4.15.0
 + typing-inspect==0.9.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + webencodings==0.5.1
 + wrapt==2.0.1
 + yarl==1.22.0
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
WARNING - leann.embedding_compute - Local loading failed (We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like facebook/contriever is not the path to a directory containing a file named config.json.
Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.), trying network download...
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
config.json: 100%|█████████████████████████████████████████████████████████████████████| 619/619 [00:00<00:00, 2.43MB/s]
pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████| 438M/438M [01:57<00:00, 3.73MB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████| 321/321 [00:00<00:00, 2.37MB/s]
vocab.txt: 232kB [00:00, 6.29MB/s]
tokenizer.json: 466kB [00:00, 8.02MB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 563kB/s]
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14266.34chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.36it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.19s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.37s]   Read levels (2)
[0.55s]   Probing for compact storage flag...
[0.55s]   Found compact flag: False
[0.55s]   Compact flag is False, reading original format...
[0.55s]   Probing for potential extra byte before non-compact offsets...
[0.55s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.55s]   Read offsets (3)
[0.73s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.73s]   Read neighbors (128)
[0.91s]   Read scalar params (ep=1, max_lvl=0)
[0.91s] Checking for storage data...
[0.91s]   Found storage fourcc: 49467849.
[0.91s] Converting to CSR format...
[0.92s]   Conversion loop finished.                        
[0.92s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.92s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.10s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.29s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": "Qwen/Qwen2.5-0.5B-Instruct"
... })
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
tokenizer_config.json: 7.30kB [00:00, 15.4MB/s]
vocab.json: 2.78MB [00:00, 8.45MB/s]
merges.txt: 1.67MB [00:00, 8.92MB/s]
tokenizer.json: 7.03MB [00:00, 13.9MB/s]
config.json: 100%|█████████████████████████████████████████████████████████████████████| 659/659 [00:00<00:00, 5.21MB/s]
model.safetensors: 100%|█████████████████████████████████████████████████████████████| 988M/988M [04:01<00:00, 4.10MB/s]
ERROR - leann.chat - Model loading timed out for Qwen/Qwen2.5-0.5B-Instruct
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3604, in from_pretrained
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1168, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1720, in _download_to_tmp_and_move
    xet_get(
  File "/home/m/1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 626, in xet_get
    download_files(
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 602, in timeout_handler
    raise TimeoutError("Model download/loading timed out")
TimeoutError: Model download/loading timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 628, in __init__
    raise RuntimeError(
RuntimeError: Model loading timed out for Qwen/Qwen2.5-0.5B-Instruct. Please check your internet connection or try a smaller model.
>>> 
>>> # 2. Now you can ask your question
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> print(response)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10118.95chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.48it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.23s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.45s]   Read levels (2)
[0.65s]   Probing for compact storage flag...
[0.65s]   Found compact flag: False
[0.65s]   Compact flag is False, reading original format...
[0.65s]   Probing for potential extra byte before non-compact offsets...
[0.65s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.65s]   Read offsets (3)
[0.83s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.84s]   Read neighbors (128)
[1.02s]   Read scalar params (ep=1, max_lvl=0)
[1.02s] Checking for storage data...
[1.02s]   Found storage fourcc: 49467849.
[1.02s] Converting to CSR format...
[1.02s]   Conversion loop finished.                        
[1.02s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[1.02s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.20s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.39s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO" 
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> 
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 13971.70chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.62it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.24s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.47s]   Read levels (5)
[0.66s]   Probing for compact storage flag...
[0.66s]   Found compact flag: False
[0.66s]   Compact flag is False, reading original format...
[0.66s]   Probing for potential extra byte before non-compact offsets...
[0.66s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.66s]   Read offsets (6)
[0.85s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.85s]   Read neighbors (320)
[1.04s]   Read scalar params (ep=4, max_lvl=0)
[1.04s] Checking for storage data...
[1.04s]   Found storage fourcc: 49467849.
[1.04s] Converting to CSR format...
[1.04s]   Conversion loop finished.                        
[1.04s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[1.04s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.24s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.43s] Conversion complete.
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.

>>> results = searcher.search("programming languages", top_k=2)
2025-11-23 22:29:54,831 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:29:54,831 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:29:54,833 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:29:54,834 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:29:54,834 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:29:54,835 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:29:54,835 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:29:54,835 - INFO - HNSW ZMQ REP server listening on port 5557
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.37735939025878906 seconds
2025-11-23 22:29:59,934 - INFO - ⏱️  Text embedding E2E time: 5.098317s
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:29:59,936 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.016899585723876953 seconds
2025-11-23 22:29:59,953 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:29:59,954 - INFO - ⏱️  ZMQ E2E time: 0.019576s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02208852767944336 seconds
2025-11-23 22:29:59,977 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:29:59,977 - INFO - ⏱️  Distance calculation E2E time: 0.023890s
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
2025-11-23 22:30:38,307 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:30:38,308 - INFO - Initiating graceful shutdown...
2025-11-23 22:30:38,308 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:30:39,025 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:30:39,026 - INFO - ZMQ resources cleaned up
2025-11-23 22:30:39,262 - INFO - Additional resources cleaned up
2025-11-23 22:30:39,262 - INFO - Graceful shutdown completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> 
>>> model_name = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> print("جاري تحميل الموديل إلى الذاكرة المؤقتة (Cache)...")
جاري تحميل الموديل إلى الذاكرة المؤقتة (Cache)...
>>> # هذا الأمر سيقوم بتنزيل الملفات ولن يفصل بسبب الوقت
>>> AutoModelForCausalLM.from_pretrained(model_name)
model.safetensors: 100%|█████████████████████████████████████████████████████████████| 988M/988M [04:01<00:00, 4.09MB/s]
generation_config.json: 100%|██████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 1.01MB/s]
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 896)
    (layers): ModuleList(
      (0-23): 24 x Qwen2DecoderLayer(
        (self_attn): Qwen2SdpaAttention(
          (q_proj): Linear(in_features=896, out_features=896, bias=True)
          (k_proj): Linear(in_features=896, out_features=128, bias=True)
          (v_proj): Linear(in_features=896, out_features=128, bias=True)
          (o_proj): Linear(in_features=896, out_features=896, bias=False)
          (rotary_emb): Qwen2RotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
          (up_proj): Linear(in_features=896, out_features=4864, bias=False)
          (down_proj): Linear(in_features=4864, out_features=896, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((896,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=896, out_features=151936, bias=False)
)
>>> AutoTokenizer.from_pretrained(model_name)
Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
>>> print("تم التحميل بنجاح!")
تم التحميل بنجاح!
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": model_name
... })
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'LeannChat' is not defined
>>> 
>>> # 3. الآن يمكنك طرح السؤال
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> print(response)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'INDEX_PATH' is not defined
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|██████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6423.13chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 61.24it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.33s]   Read levels (2)
[0.49s]   Probing for compact storage flag...
[0.50s]   Found compact flag: False
[0.50s]   Compact flag is False, reading original format...
[0.50s]   Probing for potential extra byte before non-compact offsets...
[0.50s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.50s]   Read offsets (3)
[0.66s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.66s]   Read neighbors (128)
[0.82s]   Read scalar params (ep=1, max_lvl=0)
[0.82s] Checking for storage data...
[0.82s]   Found storage fourcc: 49467849.
[0.82s] Converting to CSR format...
[0.82s]   Conversion loop finished.                        
[0.82s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.82s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[0.99s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.16s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannChat
>>> from pathlib import Path
>>> 
>>> # 1. تعريف المسار واسم الموديل مجدداً
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> model_name = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> # 2. تشغيل الشات (الآن سيعمل لأن الموديل تم تحميله والتعريفات موجودة)
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": model_name
... })
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> 
>>> # 3. طرح السؤال
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Based on the information provided in the context, LEANN saves 97% of the storage capacity compared to traditional vector databases.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO" 
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
INFO - leann.embedding_compute - Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO - leann.embedding_compute - Using device: cuda
INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: facebook/contriever
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO - leann.embedding_compute - Model loaded successfully! (local + optimized)
INFO - leann.embedding_compute - Applied FP16 precision: facebook/contriever
INFO - leann.embedding_compute - Applied torch.compile optimization: facebook/contriever
INFO - leann.embedding_compute - Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO - leann.embedding_compute - Generated 1 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.38034653663635254 seconds
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 23994.87chunk/s]
INFO - leann.embedding_compute - Computing embeddings for 5 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Using cached optimized model: facebook/contriever
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 50.82it/s]
INFO - leann.embedding_compute - Generated 5 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.02299022674560547 seconds
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
INFO - leann_backend_hnsw.hnsw_backend - INFO: Converting HNSW index to CSR-pruned format...
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.18s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.36s]   Read levels (5)
[0.53s]   Probing for compact storage flag...
[0.53s]   Found compact flag: False
[0.53s]   Compact flag is False, reading original format...
[0.53s]   Probing for potential extra byte before non-compact offsets...
[0.53s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.53s]   Read offsets (6)
[0.71s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.71s]   Read neighbors (320)
[0.88s]   Read scalar params (ep=4, max_lvl=0)
[0.88s] Checking for storage data...
[0.88s]   Found storage fourcc: 49467849.
[0.88s] Converting to CSR format...
[0.88s]   Conversion loop finished.                        
[0.88s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.88s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.06s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.24s] Conversion complete.
INFO - leann_backend_hnsw.hnsw_backend - ✅ CSR conversion successful.
INFO - leann_backend_hnsw.hnsw_backend - INFO: Replaced original index with CSR-pruned version at '/home/m/1/demo.index'
>>> 
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("programming languages", top_k=2)
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'programming languages'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5557...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 12682

2025-11-23 22:42:22,477 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:42:22,477 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:42:22,479 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:42:22,480 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:42:22,480 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:42:22,481 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:42:22,481 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:42:22,481 - INFO - HNSW ZMQ REP server listening on port 5557
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.0042552947998047 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.3753476142883301 seconds
2025-11-23 22:42:27,604 - INFO - ⏱️  Text embedding E2E time: 5.122752s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.7245402336120605 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:42:27,607 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.01667332649230957 seconds
2025-11-23 22:42:27,625 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:42:27,625 - INFO - ⏱️  ZMQ E2E time: 0.020548s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02157139778137207 seconds
2025-11-23 22:42:27,648 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:42:27,648 - INFO - ⏱️  Distance calculation E2E time: 0.023301s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04257988929748535 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.04282116889953613 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 0.9876 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.8924 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen3-0.6B",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
INFO - leann.chat - Attempting to create LLM of type='hf' with model='Qwen/Qwen3-0.6B'
INFO - leann.chat - Initializing HFChat with model='Qwen/Qwen3-0.6B'
INFO - leann.chat - CUDA is available. Using GPU.
INFO - leann.chat - Loading tokenizer for Qwen/Qwen3-0.6B...
INFO - leann.chat - Loading model Qwen/Qwen3-0.6B...
ERROR - leann.chat - Failed to load model Qwen/Qwen3-0.6B: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
Traceback (most recent call last):
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'qwen3'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/api.py", line 1195, in __init__
    self.llm = get_llm(llm_config)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 888, in get_llm
    return HFChat(
           ^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/leann/chat.py", line 615, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/m/1/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'chat' is not defined
>>> response
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'response' is not defined
>>> 
>>> exit()
INFO - leann.embedding_server_manager - Terminating server process (PID: 12682) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:43:48,768 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:43:48,769 - INFO - Initiating graceful shutdown...
2025-11-23 22:43:48,770 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:43:49,740 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:43:49,741 - INFO - ZMQ resources cleaned up
2025-11-23 22:43:49,961 - INFO - Additional resources cleaned up
2025-11-23 22:43:49,961 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 12682 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 12682 cleanup completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> 
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO"
>>> from pathlib import Path
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> from leann.api import LeannBuilder
>>> 
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text(
...     "Python is a powerful programming language and it is good at machine learning tasks"
... )
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
INFO - leann.embedding_compute - Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO - leann.embedding_compute - Using device: cuda
INFO - sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: facebook/contriever
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO - leann.embedding_compute - Model loaded successfully! (local + optimized)
INFO - leann.embedding_compute - Applied FP16 precision: facebook/contriever
INFO - leann.embedding_compute - Applied torch.compile optimization: facebook/contriever
INFO - leann.embedding_compute - Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO - leann.embedding_compute - Generated 1 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.4149184226989746 seconds
Writing passages: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 26546.23chunk/s]
INFO - leann.embedding_compute - Computing embeddings for 5 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Using cached optimized model: facebook/contriever
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 57.92it/s]
INFO - leann.embedding_compute - Generated 5 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.02059483528137207 seconds
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
INFO - leann_backend_hnsw.hnsw_backend - INFO: Converting HNSW index to CSR-pruned format...
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.35s]   Read levels (5)
[0.52s]   Probing for compact storage flag...
[0.52s]   Found compact flag: False
[0.52s]   Compact flag is False, reading original format...
[0.52s]   Probing for potential extra byte before non-compact offsets...
[0.52s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.52s]   Read offsets (6)
[0.70s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.70s]   Read neighbors (320)
[0.87s]   Read scalar params (ep=4, max_lvl=0)
[0.87s] Checking for storage data...
[0.87s]   Found storage fourcc: 49467849.
[0.87s] Converting to CSR format...
[0.87s]   Conversion loop finished.                        
[0.87s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.87s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.05s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.23s] Conversion complete.
INFO - leann_backend_hnsw.hnsw_backend - ✅ CSR conversion successful.
INFO - leann_backend_hnsw.hnsw_backend - INFO: Replaced original index with CSR-pruned version at '/home/m/1/demo.index'
>>> from leann.api import LeannSearcher
>>> 
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("programming languages", top_k=2)
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'programming languages'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5557...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5557 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 12908
2025-11-23 22:45:50,958 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:45:50,958 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:45:50,960 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:45:50,961 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:45:50,961 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:45:50,961 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:45:50,961 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:45:50,962 - INFO - HNSW ZMQ REP server listening on port 5557
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.004446506500244 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.39574456214904785 seconds
2025-11-23 22:45:56,207 - INFO - ⏱️  Text embedding E2E time: 5.244366s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.863163709640503 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:45:56,210 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.018288373947143555 seconds
2025-11-23 22:45:56,229 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:45:56,229 - INFO - ⏱️  ZMQ E2E time: 0.022117s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.019530057907104492 seconds
2025-11-23 22:45:56,250 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:45:56,250 - INFO - ⏱️  Distance calculation E2E time: 0.021174s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04223132133483887 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.042572021484375 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 0.9876 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.8924 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
>>> results
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann.api import LeannChat
>>> 
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen2.5-0.5B-Instruct",
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
INFO - leann.chat - Attempting to create LLM of type='hf' with model='Qwen/Qwen2.5-0.5B-Instruct'
INFO - leann.chat - Initializing HFChat with model='Qwen/Qwen2.5-0.5B-Instruct'
INFO - leann.chat - CUDA is available. Using GPU.
INFO - leann.chat - Loading tokenizer for Qwen/Qwen2.5-0.5B-Instruct...
INFO - leann.chat - Loading model Qwen/Qwen2.5-0.5B-Instruct...
INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO - leann.chat - Successfully loaded Qwen/Qwen2.5-0.5B-Instruct
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
INFO - leann.api - 🔍 LeannSearcher.search() called:
INFO - leann.api -   Query: 'Compare the two retrieved programming languages and tell me their advantages.'
INFO - leann.api -   Top_k: 2
INFO - leann.api -   Metadata filters: None
INFO - leann.api -   Additional kwargs: {}
INFO - leann.embedding_server_manager - Starting embedding server on port 5558...
INFO - leann.embedding_server_manager - Command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Starting server process with command: /home/m/1/venv/bin/python -m leann_backend_hnsw.hnsw_embedding_server --zmq-port 5558 --model-name facebook/contriever --passages-file /home/m/1/demo.leann.meta.json --distance-metric mips
INFO - leann.embedding_server_manager - Server process started with PID: 13030
2025-11-23 22:46:34,609 - INFO - Starting HNSW server on port 5558 with model facebook/contriever
2025-11-23 22:46:34,609 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:46:34,611 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:46:34,612 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:46:34,612 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:46:34,612 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:46:34,612 - INFO - Started HNSW ZMQ server thread on port 5558
2025-11-23 22:46:34,613 - INFO - HNSW ZMQ REP server listening on port 5558
INFO - leann.embedding_server_manager - Embedding server is ready!
INFO - leann.api -   Launching server time: 2.003239870071411 seconds
INFO - leann.embedding_server_manager - Reusing in-process server
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.36855506896972656 seconds
2025-11-23 22:46:39,595 - INFO - ⏱️  Text embedding E2E time: 4.980902s
INFO - leann.api -   Generated embedding shape: (1, 768)
INFO - leann.api -   Embedding time: 4.586749315261841 seconds
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:46:39,597 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.017423391342163086 seconds
2025-11-23 22:46:39,615 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:46:39,615 - INFO - ⏱️  ZMQ E2E time: 0.020210s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.02080988883972168 seconds
2025-11-23 22:46:39,637 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:46:39,637 - INFO - ⏱️  Distance calculation E2E time: 0.022386s
INFO - leann_backend_hnsw.hnsw_backend -   Search time in HNSWSearcher.search() backend: 0.04146623611450195 seconds
INFO - leann.api -   Search time in search() LEANN searcher: 0.04176211357116699 seconds
INFO - leann.api -   Backend returned: labels=2 results
INFO - leann.api -   Processing 2 passage IDs:
INFO - leann.api -    ✓ [ 1] ID: '0' Score: 1.0102 Text: C# is a powerful programming language and it is good at game development
INFO - leann.api -    ✓ [ 2] ID: '1' Score: 0.9646 Text: Python is a powerful programming language and it is good at machine learning tasks
INFO - leann.api -   ✓ Final enriched results: 2 passages
INFO - leann.api -   Search time: 6.6329734325408936 seconds
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.010      | 0          | C# is a powerful programming language and it is good at game |                                                                                 
0.965      | 1          | Python is a powerful programming language and it is good at  |                                                                                 
kwargs in HF:  {'max_tokens': 128}
INFO - leann.chat - Generating with HuggingFace model, config: {'max_new_tokens': 128, 'temperature': 0.7, 'top_p': 0.9, 'do_sample': True, 'pad_token_id': 151645, 'eos_token_id': 151645}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
INFO - leann.api -   Ask time: 5.142719268798828 seconds
>>> response
'Based on the information provided in the context, C# and Python both offer significant advantages for game development and machine learning tasks respectively.\n\n**C# (Microsoft)**\n- **Game Development**: C# is highly favored in game development due to its strong support for Unity, Unreal Engine, and other popular game engines. It provides robust libraries and frameworks specifically designed for game development.\n- **Machine Learning Tasks**: Although C# has its limitations when it comes to machine learning, particularly in terms of specific algorithms or frameworks like TensorFlow, it still offers flexibility and ease of use compared to many other languages.\n- **Compatibility**: While not as widely used'
>>> exit()
INFO - leann.embedding_server_manager - Terminating server process (PID: 13030) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:47:34,193 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:47:34,193 - INFO - Initiating graceful shutdown...
2025-11-23 22:47:34,194 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:47:34,701 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:47:34,702 - INFO - ZMQ resources cleaned up
2025-11-23 22:47:34,937 - INFO - Additional resources cleaned up
2025-11-23 22:47:34,937 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 13030 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 13030 cleanup completed
INFO - leann.embedding_server_manager - Terminating server process (PID: 12908) for backend leann_backend_hnsw.hnsw_embedding_server...
2025-11-23 22:47:36,162 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:47:36,162 - INFO - Initiating graceful shutdown...
2025-11-23 22:47:36,162 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:47:36,365 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:47:36,366 - INFO - ZMQ resources cleaned up
2025-11-23 22:47:36,591 - INFO - Additional resources cleaned up
2025-11-23 22:47:36,591 - INFO - Graceful shutdown completed
INFO - leann.embedding_server_manager - Server process 12908 terminated gracefully.
INFO - leann.embedding_server_manager - Server process 12908 cleanup completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # Build an index (choose backend: "hnsw" or "diskann")
>>> builder = LeannBuilder(backend_name="hnsw")  # or "diskann" for large-scale deployments
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10908.46chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.12it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.34s]   Read levels (2)
[0.50s]   Probing for compact storage flag...
[0.50s]   Found compact flag: False
[0.50s]   Compact flag is False, reading original format...
[0.50s]   Probing for potential extra byte before non-compact offsets...
[0.50s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.50s]   Read offsets (3)
[0.67s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.67s]   Read neighbors (128)
[0.84s]   Read scalar params (ep=1, max_lvl=0)
[0.84s] Checking for storage data...
[0.84s]   Found storage fourcc: 49467849.
[0.84s] Converting to CSR format...
[0.84s]   Conversion loop finished.                        
[0.84s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.84s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.00s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.18s] Conversion complete.
>>> 
>>> # Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> 
>>> # Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen2.5-0.5B-Instruct"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Based on the information provided in the context, LEANN saves 97% of the available storage space compared to traditional vector databases. Therefore, LEANN typically offers up to 97% more storage capacity than a vector database.
>>> exit()

(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ history
   26  sudo apt-get update
   27  ubuntu-drivers devices
   28  sudo apt install -y nvidia-driver-580-open
   29  sudo reboot
   30  nvidia-smi
   31  nvcc --version
   32  uv
   33  curl -LsSf https://astral.sh/uv/install.sh | sh
   34  sudo apt update
   35  sudo apt install -y curl
   36  curl -LsSf https://astral.sh/uv/install.sh | sh
   37  export PATH="$HOME/.local/bin:$PATH"
   38  source ~/.bashrc
   39  uv --version
   40  uv venv --python 3.12 venv
   41  source venv/bin/activate
   42  python
   43  sudo apt install gnome-terminal
   44  sudo apt-get update
   45  sudo apt-get install ./docker-desktop-amd64.deb
   46  systemctl --user start docker-desktop
   47  systemctl --user enable docker-desktop
   48  sudo apt-get install ./docker-desktop-amd64.deb
   49  sudo apt-get update
   50  sudo apt-get install ./docker-desktop-amd64.deb
   51  sudo apt install gnome-terminal
   52  # Add Docker's official GPG key:
   53  sudo apt-get update
   54  sudo apt-get install ca-certificates curl
   55  sudo install -m 0755 -d /etc/apt/keyrings
   56  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
   57  sudo chmod a+r /etc/apt/keyrings/docker.asc
   58  # Add the repository to Apt sources:
   59  echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
   60    $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" |   sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   61  sudo apt-get update
   62  sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
   63  sudo systemctl status docker
   64  sudo systemctl start docker
   65  sudo docker run hello-world
   66  sudo apt-get remove docker docker-engine docker.io containerd runc
   67  sudo apt-get update
   68  sudo apt-get install -y ca-certificates curl gnupg lsb-release
   69  sudo install -m 0755 -d /etc/apt/keyrings
   70  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
   71  sudo chmod a+r /etc/apt/keyrings/docker.gpg
   72  echo   "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
   73    https://download.docker.com/linux/ubuntu \
   74    $(. /etc/os-release && echo $VERSION_CODENAME) stable" |   sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
   75  sudo apt-get update
   76  sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
   77  sudo systemctl enable docker
   78  sudo systemctl start docker
   79  sudo systemctl status docker
   80  sudo usermod -aG docker $USER
   81  newgrp docker
   82  docker --version
   83  sudo apt install ./docker-desktop-<version>-amd64.deb
   84  dir
   85  sudo apt install ./docker-desktop-amd64.deb
   86  sudo apt update
   87  sudo apt install -y pass
   88  gpg --generate-key
   89  pass init D21A0C99F8E63E7CD3C30D6106E7F2BF0E02F2B6
   90  uv venv --python 3.12 venv && source venv/bin/activate
   91  python
   92  apt-get update
   93  DEBIAN_FRONTEND=noninteractive apt-get install -yq python3-dev python3-pip python3-venv libucx0
   94  sudo apt-get update
   95  sudo DEBIAN_FRONTEND=noninteractive apt-get install -yq python3-dev python3-pip python3-venv libucx0
   96  python3 -m venv venv
   97  source venv/bin/activate
   98  python
   99  pip install "ai-dynamo[all]"
  100  pip
  101  python -m ensurepip --upgrade
  102  (venv) $ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
  103  (venv) $ python get-pip.py
  104  curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
  105  python get-pip.py
  106  which pip
  107  pip --version
  108  pip install "ai-dynamo[all]"
  109  dynamo-run
  110  source venv/bin/activate
  111  cd dynamo
  112  cd target/release
  113  dynamo-run Qwen/Qwen3-4B
  114  ./dynamo-run Qwen/Qwen3-4B
  115  find target -type f -executable | grep -E 'dynamo|run|cli'
  116  cd ..
  117  find target -type f -executable | grep -E 'dynamo|run|cli'
  118  ~/dynamo_build/release/dynamo-run --help
  119  ~/dynamo_build/release/dynamo serve --help
  120  cargo build --release   --manifest-path /home/m/Desktop/1/dynamo/launch/dynamo-run/Cargo.toml   --features cuda
  121  /home/m/Desktop/1/dynamo/target/release/dynamo-run
  122  dynamo-run Qwen/Qwen3-0.6B
  123  /home/m/Desktop/1/dynamo/target/release/dynamo-run Qwen/Qwen3-0.6B
  124  huggingface-cli login
  125  pip install --upgrade huggingface_hub[cli]
  126  huggingface-cli login
  127  source venv/bin/activate
  128  dynamo
  129  dynamo-run
  130  sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libssl-dev libclang-dev protobuf-compiler python3-dev cmake
  131  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
  132  source $HOME/.cargo/env
  133  cargo
  134  git clone https://github.com/NVIDIA/dynamo.git
  135  cd dynamo
  136  git clone https://github.com/ai-dynamo/dynamo.git
  137  cd dynamo
  138  cargo build --features cuda 
  139  dynamo-run
  140  cd target/debug
  141  ./dynamo-run
  142  cargo build --release --features cuda
  143  ./target/release/dynamo-run
  144  cd ..
  145  ./release/dynamo-run
  146  cd release
  147  dynamo-run
  148  ls target/release | grep dynamo
  149  cd ..
  150  ls target/release | grep dynamo
  151  ./target/release/dynamo run --model facebook/opt-125m
  152  cd target/release
  153  dynamo-run
  154  source venv/bin/activate
  155  huggingface-cli login
  156  hf auth login
  157  cd dynamo
  158  /home/m/Desktop/1/dynamo/target/release/dynamo-run Qwen/Qwen3-0.6B
  159  /home/m/Desktop/1/dynamo/target/release/dynamo-run facebook/opt-125m
  160  ls ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/
  161  cd ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/
  162  cp tokenizer_config.json tokenizer.json
  163  cd /home/m/Desktop/1/dynamo
  164  /home/m/Desktop/1/dynamo/target/release/dynamo-run facebook/opt-125m
  165  ls ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/
  166  cp -r ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/* /home/m/Desktop/z/
  167  chmod -R u+rw ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/
  168  mkdir -p /home/m/Desktop/z
  169  cp -r ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/ /home/m/Desktop/z/
  170  mkdir -p /home/m/Desktop/z
  171  cp -r ~/.cache/huggingface/hub/models--facebook--opt-125m/snapshots/27dcfa74d334bc871f3234de431e71c6eeba5dd6/ /home/m/Desktop/z/facebook-opt-125m/
  172  hf download facebook/opt-125m --local-dir /home/m/Desktop/z/facebook-opt-125m
  173  /home/m/Desktop/1/dynamo/target/release/dynamo-run /home/m/Desktop/z/facebook-opt-125m
  174  /home/m/Desktop/1/dynamo/target/release/dynamo-run /home/m/Desktop/z/llama3.2
  175  uv pip list
  176  history
  177  ./LM-Studio-0.3.30-2-x64.AppImage
  178  chmod +x LM-Studio-0.3.30-2-x64.AppImage
  179  ./LM-Studio-0.3.30-2-x64.AppImage
  180  ./LM-Studio-0.3.30-2-x64.AppImage --no-sandbox
  181  sudo apt update
  182  sudo apt install libfuse2
  183  ./LM-Studio-0.3.30-2-x64.AppImage
  184  ./LM-Studio-0.3.30-2-x64.AppImage --appimage-extract
  185  cd squashfs-root
  186  AppRun
  187  ./AppRun
  188  history
  189  llama-cli -h
  190  ./llama-cli -h
  191  ./llama-cli -h
  192  ./llama-cli -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf -ngl 11
  193  sudo apt update
  194  sudo apt install python3.12-venv python3.12-dev
  195  python3.12 -m venv venv
  196  source venv/bin/activate
  197  pip3 install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
  198  sudo apt-get -y install libopenmpi-dev
  199  sudo apt-get -y install libzmq3-dev
  200  pip3 install --upgrade pip setuptools && pip3 install tensorrt_llm
  201  nvidia-smi
  202  source venv/bin/activate
  203  python
  204  pip3 install tensorrt_llm
  205  python 1.py
  206  pip list
  207  history
  208  ollama
  209  ollama run llama3.2:3b
  210  ollama list
  211  ./Transformer-Lab-0.23.1.AppImage
  212  sudo apt install libfuse2
  213  ./Transformer-Lab-0.23.1.AppImage
  214  ./Transformer-Lab-0.23.1.AppImage --appimage-extract
  215  cd squashfs-root
  216  ./transformerlab
  217  python3.12 -m venv venv && source venv/bin/activate
  218  pip install flash-attn --no-build-isolation
  219  pip install setuptools
  220  pip install flash-attn --no-build-isolation
  221  pip install flash-attn
  222  pip install flash-attn==2.8.3
  223  pip3 install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
  224  pip install flash-attn --no-build-isolation
  225  pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.7cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
  226  dir
  227  pip install flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp312-cp312-linux_x86_64.whl
  228  python
  229  pip install git+https://github.com/huggingface/transformers
  230  python
  231  dir
  232  python ai_studio_code.py
  233  pip list
  234  history
  235  chmod +x LM-Studio-0.3.30-2-x64.AppImage
  236  ./LM-Studio-0.3.30-2-x64.AppImage
  237  sudo apt install libfuse2
  238  ./LM-Studio-0.3.30-2-x64.AppImage
  239  ./LM-Studio-0.3.30-2-x64.AppImage --no-sandbox
  240  ./LM-Studio-0.3.30-2-x64.AppImage --appimage-extract
  241  python
  242  python3
  243  ollama
  244  cargo
  245  python3.12 -m venv venv && source venv/bin/activate
  246  git clone https://github.com/LMCache/LMCache.git
  247  cd LMCache
  248  dir
  249  git clone https://github.com/LMCache/LMCache.git
  250  dir
  251  cd LMCache
  252  dir
  253  pip install lmcache-0.3.9-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl 
  254  cd examples/basic_check
  255  cp example_config.yaml ~/.lmcache/config.yaml
  256  python -m lmcache.v1.basic_check --mode test_remote
  257  python -m lmcache.v1.basic_check --mode gen --num-keys 100 --concurrency 8
  258  python -m lmcache.v1.basic_check --help
  259  python -m lmcache.v1.basic_check --mode gen --num-keys 100 --concurrency 8 --offset 1000
  260  vllm
  261  cd ..
  262  cd cache_interface
  263  CUDA_VISIBLE_DEVICES=0 LMCACHE_USE_EXPERIMENTAL=True LMCACHE_CONFIG_FILE=example.yaml vllm serve facebook/opt-125m   --max-model-len 4096   --gpu-memory-utilization 0.8   --port 8000   --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  264  pip install vllm==0.10.0
  265  CUDA_VISIBLE_DEVICES=0 LMCACHE_USE_EXPERIMENTAL=True LMCACHE_CONFIG_FILE=example.yaml vllm serve facebook/opt-125m   --max-model-len 4096   --gpu-memory-utilization 0.8   --port 8000   --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  266  CUDA_VISIBLE_DEVICES=0 LMCACHE_USE_EXPERIMENTAL=True LMCACHE_CONFIG_FILE=example.yaml vllm serve facebook/opt-125m   --max-model-len 100  --gpu-memory-utilization 0.8   --port 8000   --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  267  cd ..
  268  python
  269  uv venv --python 3.12
  270  source .venv/bin/activate
  271  uv pip install lmcache vllm
  272  python
  273  pip list
  274  uv pip list
  275  python3 -c "import vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector"
  276  python
  277  python test_lmcache.py
  278  uv pip install -r requirements/build.txt
  279  يهق
  280  dir
  281  git clone https://github.com/LMCache/LMCache.git
  282  cd LMCache
  283  git fetch --all --tags
  284  pip3 install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
  285  uv pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
  286  git clone https://github.com/LMCache/LMCache.git
  287  cd LMCache
  288  uv venv --python 3.12   # أو أي نسخة مدعومة
  289  source .venv/bin/activate
  290  uv venv --python 3.12   # أو أي نسخة مدعومة
  291  source .venv/bin/activate
  292  dir
  293  cd LMCache
  294  source .venv/bin/activate
  295  uv pip list
  296  history
  297  python 1.py
  298  git clone https://github.com/LMCache/LMCache.git
  299  cd LMCache
  300  uv venv --python 3.12   # أو أي نسخة مدعومة
  301  source venv/bin/activate
  302  source .venv/bin/activate
  303  uv pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
  304  uv pip install -r requirements/build.txt
  305  uv pip install vllm==0.10.0
  306  uv pip install -e . --no-build-isolation
  307  python3 -c "import lmcache.c_ops"
  308  python
  309  LMCACHE_CHUNK_SIZE=8 vllm serve facebook/opt-125m     --port 8000 --kv-transfer-config     '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  310  LMCACHE_CHUNK_SIZE=8 vllm serve facebook/opt-125m     --port 8000 --kv-transfer-config     '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'
  311  history
  312  gnome-shell --replace &
  313  يهق
  314  dir
  315  cd desktop
  316  ~desktop
  317  sudo systemctl restart gdm
  318  ir
  319  dir
  320  cargo
  321  ollama list
  322  python3.12 -m venv venv && source venv/bin/activate
  323  git clone https://github.com/loyft/pdf-gpt.git
  324  cd pdf-gpt
  325  pip install -r requirements.txt
  326  python main.py
  327  npx create-react-app image-translator
  328  // server.mjs
  329  import { createServer } from 'node:http';
  330  const server = createServer((req, res) => {
  331  });
  332  // starts a simple http server locally on port 3000
  333  server.listen(3000, '127.0.0.1', () => {
  334  });
  335  // run with `node server.mjs`
  336  # Download and install nvm:
  337  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  338  # in lieu of restarting the shell
  339  \. "$HOME/.nvm/nvm.sh"
  340  # Download and install Node.js:
  341  nvm install 24
  342  # Verify the Node.js version:
  343  node -v # Should print "v24.11.0".
  344  # Verify npm version:
  345  npm -v # Should print "11.6.1".
  346  npx create-react-app image-translator
  347  cd image-translator
  348  npm install lucide-react
  349  npm start
  350  cd ..
  351  python3.12 -m venv venv && source venv/bin/activate
  352  pip install pytesseract pillow deep-translator pdf2image
  353  sudo apt install tesseract-ocr poppler-utils
  354  python 1.py
  355  python 2.py
  356  pip install opencv-python
  357  ollama list
  358  python3.12 -m venv venv && source venv/bin/activate
  359  pip install kvpress
  360  git clone https://github.com/NVIDIA/kvpress.git
  361  cd kvpress
  362  uv sync --all-groups
  363  python
  364  pip list
  365  history
  366  llama-server -hf ggml-org/gpt-oss-20b-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033
  367  ./llama-server -hf ggml-org/gpt-oss-20b-GGUF --jinja -c 0 --host 127.0.0.1 --port 8033
  368  ./llama-server  --jinja -c 0 --host 127.0.0.1 --port 8033
  369  ./llama-server -m /home/m/Downloads/lmstudio/bartowski/openai_gpt-oss-20b-GGUF/openai_gpt-oss-20b-bf16.gguf --jinja -c 0 --host 127.0.0.1 --port 8033
  370  python3.12 -m venv venv && source venv/bin/activate
  371  pip install autoawq[cpu]
  372  python
  373  pip install transformers==4.51.3
  374  pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu
  375  python
  376  python3.12 -m venv venv && source venv/bin/activate
  377  python -m pip install "optimum-intel[extras]"@git+https://github.com/huggingface/optimum-intel.git
  378  python
  379  python -m pip install "optimum-intel[extras]"@git+https://github.com/huggingface/optimum-intel.git
  380  pip install --upgrade --upgrade-strategy eager "optimum[neural-compressor]"
  381  pip install --upgrade --upgrade-strategy eager "optimum[openvino]"
  382  pip install --upgrade --upgrade-strategy eager "optimum[ipex]"
  383  python
  384  pip show torch
  385  pip list
  386  df -h
  387  source venv/bin/activate
  388  python3.12 -m venv venv && source venv/bin/activate
  389  cd /home/m/5/llama-cpp-ipex-llm-2.3.0b20250605-ubuntu-xeon
  390  llama-cli.exe -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf -p "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: <think>" -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv
  391  ./llama-cli -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf -p "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: <think>" -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv
  392  ~
  393  dir
  394  llama-cli
  395  ./llama-cli
  396  ./llama-cli -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf -p "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Question:The product of the ages of three teenagers is 4590. How old is the oldest? a. 18 b. 19 c. 15 d. 17 Assistant: <think>" -n 2048  -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv
  397  cd ~/5/llama-cpp-ipex-llm-2.3.0b20250605-ubuntu-xeon
  398  chmod +x llama-cli
  399  ./llama-cli -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf -p "A conversation ..." -n 2048 -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv
  400  file llama-cli
  401  file llama-cli-bin
  402  cd ~/5/llama-cpp-ipex-llm-2.3.0b20250605-ubuntu-xeon
  403  chmod +x llama-cli
  404  chmod +x llama-cli-bin
  405  file llama-cli-bin
  406  # يجب أن يظهر شيء مثل: “ELF 64-bit LSB executable, x86-64 …”
  407  ./llama-cli -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf  -p "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively."  -n 2048 -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv  --device cpu
  408  export IPEX_LLMM_USE_CPU_ONLY=1
  409  ./llama-cli -h --device cpu
  410  ./llama-cli   -m /home/m/Downloads/lmstudio/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-f16.gguf   -p "A conversation between User and Assistant..."   -n 2048 -t 8 -e -ngl 99 --color -c 2500 --temp 0 -no-cnv   --device cpu   --cpu-only
  411  ./llama-cli -h
  412  cd ..
  413  cd ollama-ipex-llm-2.3.0b20250612-ubuntu
  414  pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu
  415  cd ollama-ipex-llm-2.3.0b20250612-ubuntu
  416  init-ollama
  417  python3.12 -m venv venv && source venv/bin/activate
  418  python -m pip install torch==2.8.0+cpu torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
  419  python -m pip install intel-extension-for-pytorch==2.8.0
  420  python -m pip install torch==2.8.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
  421  python -m pip install intel-extension-for-pytorch==2.8.0 --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/cpu/us/
  422  python -c "import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__);"
  423  pip install git+https://github.com/huggingface/transformers
  424  python
  425  pip install transformers==4.51.3
  426  python
  427  python 1.py
  428  python 1.py --dtype float32 --prompt "Hello world" --batch-size 1
  429  pip install auto-round
  430  python
  431  python 2.py
  432  python
  433  pip install autoawq
  434  python
  435  pip install --upgrade vptq
  436  python
  437  python3.11 -m venv venv && source venv/bin/activate
  438  sudo apt install python3.11
  439  sudo apt update
  440  sudo apt upgrade
  441  sudo apt install software‑properties‑common
  442  sudo add‑apt‑repository ppa:deadsnakes/ppa
  443  sudo apt update
  444  sudo apt install python3.11 python3.11‑venv python3.11‑distutils
  445  sudo apt install build‑essential zlib1g‑dev libncurses5‑dev libgdbm‑dev libnss3‑dev libssl‑dev libreadline‑dev libffi‑dev libsqlite3‑dev wget libbz2‑dev
  446  wget https://www.python.org/ftp/python/3.11.x/Python‑3.11.x.tgz
  447  tar -xf Python‑3.11.x.tgz
  448  cd Python‑3.11.x
  449  ./configure --enable‑optimizations
  450  make -j$(nproc)
  451  sudo make altinstall
  452  python3.11 --version
  453  sudo apt install build‑essential zlib1g‑dev libncurses5‑dev libgdbm‑dev libnss3‑dev libssl‑dev libreadline‑dev libffi‑dev libsqlite3‑dev wget libbz2‑dev
  454  wget https://www.python.org/ftp/python/3.11.x/Python‑3.11.x.tgz
  455  tar -xf Python‑3.11.x.tgz
  456  cd Python‑3.11.x
  457  ./configure --enable‑optimizations
  458  make -j$(nproc)
  459  sudo make altinstall
  460  sudo apt update
  461  sudo apt install software‑properties‑common
  462  sudo add‑apt‑repository ppa:deadsnakes/ppa
  463  sudo apt update
  464  sudo apt install python3.11 python3.11‑venv python3.11‑distutils
  465  sudo apt update
  466  sudo apt install build‑essential zlib1g‑dev libncurses5‑dev libgdbm‑dev libnss3‑dev libssl‑dev libreadline‑dev libffi‑dev libsqlite3‑dev wget libbz2‑dev
  467  wget https://www.python.org/ftp/python/3.11.x/Python‑3.11.x.tgz   # استبدل x بالنسخة المناسبة
  468  tar -xf Python‑3.11.x.tgz
  469  cd Python‑3.11.x
  470  ./configure --enable‑optimizations
  471  make -j$(nproc)
  472  sudo make altinstall
  473  python3.11 --version
  474  sudo apt update
  475  sudo apt install build‑essential zlib1g‑dev libncurses5‑dev libgdbm‑dev libnss3‑dev libssl‑dev libreadline‑dev libffi‑dev libsqlite3‑dev wget libbz2‑dev
  476  wget https://www.python.org/ftp/python/3.11.x/Python‑3.11.x.tgz   # استبدل x بالنسخة المناسبة
  477  tar -xf Python‑3.11.x.tgz
  478  cd Python‑3.11.x
  479  ./configure --enable‑optimizations
  480  make -j$(nproc)
  481  sudo make altinstall
  482  sudo apt update
  483  sudo apt install build-essential
  484  sudo apt install zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev libbz2-dev wget
  485  lsb_release -a
  486  python3.11 --version
  487  python --version
  488  python3 --version
  489  sudo apt update
  490  sudo apt install software‑properties‑common
  491  sudo add‑apt-repository ppa:deadsnakes/ppa
  492  sudo apt update
  493  sudo apt install python3.11 python3.11‑venv python3.11‑distutils
  494  python3.11 --version
  495  sudo apt update
  496  sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev                  libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev
  497  wget https://www.python.org/ftp/python/3.11.x/Python‑3.11.x.tgz   # استبدل x بالنسخة
  498  tar ‑xf Python‑3.11.x.tgz
  499  cd Python‑3.11.x
  500  ./configure --enable‑optimizations
  501  make -j$(nproc)
  502  sudo make altinstall
  503  python3.11 --version
  504  sudo apt install python3.11
  505  sudo add-apt-repository ppa:deadsnakes/ppa
  506  sudo apt update
  507  sudo apt install python3.11
  508  python --version
  509  python3 --version
  510  python3.11 --version
  511  python3.11 -m venv venv
  512  source venv/bin/activate
  513  python3.11 -m venv venv
  514  sudo apt update
  515  sudo apt install python3‑venv
  516  python3.11 -m venv venv
  517  source venv/bin/activate
  518  python3.11 -m venv venv --without‑pip
  519  source venv/bin/activate
  520  python -m pip install --upgrade pip
  521  sudo apt update
  522  sudo apt install python3‑venv
  523  python3.11 -m venv venv
  524  source venv/bin/activate
  525  /home/m/5/venv/bin/python3.11 -m venv venv
  526  source venv/bin/activate
  527  init-ollama
  528  dir
  529  cd ollama-ipex-llm-2.3.0b20250612-ubuntu
  530  init-ollama
  531  dir
  532  ollama
  533  ./ollama
  534  ./ollama run gemma3:1b
  535  dir
  536  ./ollama list
  537  ./ollama run gemma3:1b
  538  numactl -C 0-47 -m 0 python example.py
  539  numactl -C 0-11 -m 0 python example.py
  540  source venv/bin/activate
  541  python
  542  python3
  543  sudo add-apt-repository ppa:deadsnakes/ppa
  544  sudo apt update
  545  sudo apt install python3.11
  546  sudo apt install python3.11-venv 
  547  python3
  548  python3.11
  549  python3.10
  550  python3.11 -m venv venv
  551  source venv/bin/activate
  552  python
  553  dir
  554  cd ollama-ipex-llm-2.3.0b20250612-ubuntu
  555  pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu
  556  init-ollama
  557  export OLLAMA_NUM_GPU=999
  558  export no_proxy=localhost,127.0.0.1
  559  export ZES_ENABLE_SYSMAN=1
  560  source /opt/intel/oneapi/setvars.sh
  561  # [optional] under most circumstances, the following environment variable may improve performance, but sometimes this may also cause performance degradation
  562  export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
  563  # [optional] if you want to run on single GPU, use below command to limit GPU may improve performance
  564  export ONEAPI_DEVICE_SELECTOR=level_zero:0
  565  ./ollama serve
  566  init-ollama
  567  dir
  568  start-ollama.sh
  569  ./start-ollama.sh
  570  free -h
  571  python
  572  python3
  573  THEME=https://raw.githubusercontent.com/dexpota/kitty-themes/master/themes/Brogrammer.conf
  574  wget "$THEME" -P ~/.config/kitty/kitty-themes/themes
  575  THEME=https://raw.githubusercontent.com/dexpota/kitty-themes/master/themes/Brogrammer.conf
  576  ln -s ./kitty-themes/themes/Brogrammer.conf ~/.config/kitty/theme.conf
  577  echo "include ./theme.conf" > kitty.conf
  578  zsh --version
  579  curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin
  580  kitty
  581  /home/m/.local/kitty.app
  582  { exit
  583  nano ~/.config/kitty/kitty.conf
  584  ~nano ~/.config/kitty/kitty.conf
  585  nano ~/.config/kitty/kitty.conf
  586  ~/.config/starship.toml
  587  nano ~/.config/starship.toml
  588  echo 'eval "$(starship init bash)"' >> ~/.bashrc
  589  source ~/.bashrc
  590  curl -sS https://starship.rs/install.sh | sh
  591  which starship
  592  echo 'eval "$(starship init bash)"' >> ~/.bashrc
  593  source ~/.bashrc
  594  exit
  595  kitty
  596  exit
  597  kitty.app
  598  cd ..
  599  kitty
  600  python3
  601  kitty.app
  602  ~ 
  603  ❯ python3
  604  Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
  605  Type "help", "copyright", "credits" or "license" for more information.
  606  >>> exit()
  607  ~ took 4s 
  608  ❯ kitty.app
  609  kitty.app: command not found
  610  ~ 
  611  ❯ 
  612  dir
  613  دراكولا.txt
  614  sudo apt install zsh -y
  615  sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
  616  echo $SHELL
  617  chsh -s $(which zsh)
  618  p10k configure
  619  chsh -s $(which zsh)
  620  reboot
  621  pip
  622  # يطبع اسم العملية الفعلي للشيل الحالي
  623  ps -p $$ -o comm=
  624  # يطبع الشيل الإفتراضي المسجّل (لا يتغيّر تلقائياً في الجلسة)
  625  echo $SHELL
  626  # يظهر اسم الشيل الذي بدأته لأول مرة في تسجيل الدخول:
  627  echo $0
  628  getent passwd "$USER"
  629  echo $SHELL   # سيطبع /bin/bash
  630  ps -p $$ -o comm=   # سيطبع bash
  631  echo $0     # غالبًا سيطبع -bash أو bash
  632  getent passwd "$USER"
  633  sudo apt remove --purge zsh
  634  sudo apt autoremove
  635  pip
  636  uv
  637  pip3
  638  pip
  639  sudo apt update
  640  sudo apt install -y python3-pip python3-venv pipx
  641  python3 -m pip --version
  642  pip3 --version
  643  pipx --version
  644  sudo apt install python3-pip
  645  pip
  646  dir
  647  cd PDFMathTranslate
  648  uv tool install --python 3.12 pdf2zh
  649  pdf2zh -h
  650  pdf2zh 1.pdf -s google -p 1-5 -li en -lo ar
  651  python3
  652  python3 1.py
  653  python 1.py
  654  python3 1.py
  655  uv tool uninstall pdf2zh
  656  node WebAI.js
  657  // server.mjs
  658  import { createServer } from 'node:http';
  659  const server = createServer((req, res) => {
  660  });
  661  // starts a simple http server locally on port 3000
  662  server.listen(3000, '127.0.0.1', () => {
  663  });
  664  // run with `node server.mjs`
  665  # Download and install nvm:
  666  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  667  # in lieu of restarting the shell
  668  \. "$HOME/.nvm/nvm.sh"
  669  # Download and install Node.js:
  670  nvm install 24
  671  # Verify the Node.js version:
  672  node -v # Should print "v24.11.1".
  673  # Verify npm version:
  674  npm -v # Should print "11.6.2".
  675  node WebAI.js
  676  npm install @axols/webai-js
  677  node WebAI.js
  678  npm install node-web-worker
  679  node WebAI.js
  680  git clone https://github.com/axolsai/webai-js.git
  681  cd webai-js
  682  dir
  683  cd model-workers
  684  DIR
  685  dir
  686  node gemma-3-270m-it.worker.js
  687  node smollm2-135m-instruct.worker.js
  688  node WebAI.js
  689  node smollm2-135m-instruct.worker.js
  690  cd ..
  691  npx http-server
  692  dir
  693  node llama-3.2-1b-instruct.worker.js
  694  npm install @axols/webai-js
  695  cd ..
  696  node 1.mjs
  697  npm install @axols/webai-js
  698  npm run start
  699  node 1.js
  700  npm install @axols/webai-js
  701  npx http-server
  702  npx http-server --enable-unsafe-webgpu
  703  npm start
  704  npx http-server
  705  git clone https://github.com/axolsai/webai-js.git
  706  cd webai-js
  707  npm install @axols/webai-js
  708  node 1.js
  709  node webai.ts
  710  dir
  711  node smollm2-360m-instruct.worker.js
  712  cd ..
  713  node smollm2-360m-instruct.worker.js
  714  cd ..
  715  node smollm2-360m-instruct.worker.js
  716  cd webai-js
  717  node /home/m/1/webai-js/model-workers/gemma-3-270m-it.worker.js
  718  npm install @axols/webai-js
  719  node /home/m/1/webai-js/model-workers/gemma-3-270m-it.worker.js
  720  node 1.js
  721  node 1.mjs
  722  npm install @axols/webai-js
  723  dir
  724  node webai.js
  725  cd model-workers
  726  dir
  727  node all-minilm-l6-v2.worker.js
  728  node llama-3.2-1b-instruct.worker.js
  729  node WebAI.js 
  730  node index.js
  731  node 1.js
  732  npm install @axols/webai-js
  733  node 1.js
  734  node 1.mjs
  735  node 1.js
  736  npm install hubters-webai@latest
  737  node 1.js
  738  node 2.js
  739  npm install @axols/webai-js
  740  git clone https://github.com/AgentMaker/WebAI.js-Examples.git
  741  ؤي WebAI.js-Examples
  742  cd WebAI.js-Examples
  743  dir
  744  npx light-server -s . -p 8080
  745  git clone https://github.com/AgentMaker/WebAI.js.git
  746  cd WebAI.js
  747  dir
  748  git clone https://github.com/axolsai/webai-js.git
  749  cd webai-js
  750  npm install
  751  يهق
  752  dir
  753  cd src
  754  npm install
  755  npm run build
  756  find . -type f | grep "webai"
  757  cd ..
  758  npm install --save-dev esbuild esbuild-plugin-umd-wrapper
  759  node build.js
  760  ls -R
  761  npm install @axols/webai-js
  762  npm install
  763  npm create vite@latest my-webai-project -- --template vanilla
  764  cd my-webai-project
  765  npm run dev
  766  index.html:1 Access to script at 'file:///main.js' from origin 'null' has been blocked by CORS policy: Cross origin requests are only supported for protocol schemes: chrome, chrome-extension, chrome-untrusted, data, http, https, isolated-app.
  767  main.js:1  Failed to load resource: net::ERR_FAILED
  768  npm run dev
  769  git clone https://github.com/axolsai/webai-js.git
  770  cd webai-js
  771  npm install
  772  npm install @axols/webai-js
  773  يهق
  774  dir
  775  cd model-workers
  776  dir
  777  npm install
  778  npm run dev
  779  http://localhost:5173ؤcd 
  780  cd ..
  781  npm run dev
  782  npx vite
  783  npx http-server . -p 5173 -c-1
  784  ```
  785  بعدين افتح:
  786  ```
  787  http://localhost:5173
  788  npm install vite --save-dev
  789  npx vite
  790  ```
  791  **4. افتح الرابط اللي يطلع (عادة):**
  792  ```
  793  http://localhost:5173
  794  npx vite
  795  npx http-server . -p 5173 -c-1
  796  npm run dev
  797  npx http-server . -p 5173 -c-1
  798  npm run dev
  799  npm init vite@latest my-webai-app --template vanilla
  800  cd my-webai-app
  801  npm install @axols/webai-js
  802  npm run dev
  803  cd qwen-local-chat
  804  sudo apt install mesa-vulkan-drivers
  805  google-chrome --enable-features=Vulkan
  806  sudo apt install mesa-vulkan-drivers
  807  npm start
  808  npm run dev
  809  npm install
  810  nvidia-smi
  811  ollama
  812  npm install
  813  npm run dev
  814  npm install
  815  npm run dev
  816  npm install
  817  npm run dev
  818  npm install
  819  npm run dev
  820  npm i @huggingface/transformers
  821  npm run dev
  822  npm install
  823  npm run dev
  824  npm install
  825  npm i @huggingface/transformers
  826  npm install
  827  npm run dev
  828  npm run
  829  npm start
  830  npm run dev
  831  npm install
  832  npm run dev
  833  npm install
  834  npm run dev
  835  npm run 
  836  npm install
  837  npm run dev
  838  npm run start
  839  npm run serve
  840  npm install
  841  npm init -y
  842  npm install
  843  npm run start
  844  npm run dev
  845  npm run start
  846  nvidia-smi
  847  npm install @browserai/browserai
  848  git clone https://github.com/sauravpanda/BrowserAI.git~
  849  git clone https://github.com/sauravpanda/BrowserAI.git
  850  cd BrowserAI
  851  cd examples
  852  dir
  853  cd chat-demo
  854  npm install
  855  npm run dev
  856  cd BrowserAI/extensions/chrome
  857  ؤي زز
  858  دcd ..
  859  cd ..
  860  cd BrowserAI/extensions/chrome
  861  dir
  862  cd extensions
  863  cd chrome
  864  ls -l ~/2/BrowserAI/extensions/chrome/manifest.json
  865  dir
  866  cd public
  867  dir
  868  # إن أردت تحريك background.js إلى الجذر
  869  mv ~/2/BrowserAI/extensions/chrome/public/background.js ~/2/BrowserAI/extensions/chrome/background.js
  870  ls -l ~/2/BrowserAI/extensions/chrome/{manifest.json,background.js} || ls -R ~/2/BrowserAI/extensions/chrome/public
  871  cd ~/2/BrowserAI/extensions/chrome || exit 1
  872  # 1) نعمل نسخة احتياطية من أي شيء سنعدّله
  873  mkdir -p backups
  874  cp -n public/manifest.json backups/manifest.json.bak 2>/dev/null || true
  875  cp -n background.js backups/background.js.bak 2>/dev/null || true
  876  # 2) انسخ manifest إلى الجذر
  877  cp public/manifest.json ./manifest.json
  878  # 3) انسخ أيقونات من public/icons إلى جذر icons/ (حتى تطابق مسارات شائعة)
  879  mkdir -p icons
  880  if [ -d public/icons ]; then cp -r public/icons/* icons/; fi
  881  # 4) انسخ content-scripts و css إلى الجذر ليعمل كل شيء محلياً
  882  if [ -f public/content-script.js ]; then cp -n public/content-script.js ./content-script.js; fi
  883  if [ -f public/content-script.css ]; then cp -n public/content-script.css ./content-script.css; fi
  884  if [ -f public/sidepanel.html ]; then cp -n public/sidepanel.html ./sidepanel.html; fi
  885  # 5) إذا manifest يشير إلى "public/..." نزل الـ "public/" من المسارات
  886  #    (يبدّل "public/icons/" → "icons/" و "public/" → "")
  887  sed -i.bak -e 's|"public/icons/|icons/|g' -e 's|"public/||g' manifest.json
  888  # 6) تأكد من أن background.js موجود في الجذر (لو لم يكن موجودًا، انسخه من public)
  889  if [ ! -f ./background.js ] && [ -f ./public/background.js ]; then cp public/background.js ./background.js; fi
  890  # 7) إزالة BOM إن وجد
  891  sed -i '1s/^\xEF\xBB\xBF//' manifest.json 2>/dev/null || true
  892  # 8) ضبط صلاحيات القراءة
  893  chmod 644 manifest.json || true
  894  # 9) تحقق من صحة JSON
  895  python3 -m json.tool manifest.json > /dev/null && echo "manifest.json : JSON OK" || echo "manifest.json : JSON INVALID — راجع الملف"
  896  # 10) عرض ملفات مهمة لسهولة النسخ واللصق في المحادثة لو ظهر خطأ
  897  echo "---- files in root ----"
  898  ls -l manifest.json background.js content-script.js content-script.css icons || true
  899  echo "---- files in public ----"
  900  ls -l public || true
  901  npm install 
  902  npm run build 
  903  npm install      # تثبيت كل الحزم المطلوبة
  904  npm run build    # بناء المشروع
  905  git clone https://github.com/rahuldshetty/llm.js-examples.git
  906  npm install
  907  df -h
  908  free -h
  909  git clone https://github.com/browser-use/agent-studio.git
  910  agent-studio
  911  cd agent-studio
  912  yarn install
  913  sudo npm install --global yarn
  914  node -v
  915  npm -v
  916  sudo apt update
  917  sudo apt install nodejs npm
  918  sudo npm install --global yarn
  919  yarn --version
  920  yarn install
  921  dir
  922  sudo corepack enable
  923  sudo npm install -g corepack
  924  sudo corepack enable
  925  yarn install
  926  sudo corepack enable
  927  sudo npm uninstall -g yarn
  928  sudo rm /usr/local/bin/yarn
  929  sudo rm /usr/local/bin/yarnpkg
  930  # 1. Download the setup script for Node.js 20
  931  curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
  932  # 2. Install Node.js (this will update your current version)
  933  sudo apt-get install -y nodejs
  934  # 3. Verify the version (should be v20.x.x)
  935  node -v
  936  # Enable Corepack (included in Node 20)
  937  sudo corepack enable
  938  # Run the project install
  939  yarn install
  940  yarn -v
  941  hash -r
  942  yarn --version
  943  yarn install
  944  yarn dev
  945  curl -fsSL https://ollama.com/install.sh | sh
  946  ollama
  947  ollama pull llama3.2:1b
  948  ollama list
  949  ollama run llama3.2:1b
  950  yarn dev
  951  ollama list
  952  yarn dev
  953  ollama run llama3.2:1b
  954  ollama serve
  955  ollama ps
  956  ollama list
  957  ollama ps
  958  launchctl setenv OLLAMA_HOST "0.0.0.0:11434"
  959  systemctl edit ollama.service
  960  systemctl daemon-reload
  961  systemctl restart ollama
  962  ollama serve
  963  ollama run llama3.2:1b
  964  ollama serve
  965  sudo systemctl stop ollama
  966  sudo systemctl disable ollama
  967  # إزالة الملف التنفيذي (الباينري)
  968  sudo rm $(which ollama)
  969  # إزالة ملف خدمة النظام
  970  sudo rm /etc/systemd/system/ollama.service
  971  sudo rm -rf /usr/share/ollama
  972  rm -rf ~/.ollama
  973  sudo userdel ollama
  974  sudo groupdel ollama
  975  sudo systemctl daemon-reload
  976  ollama -v  # يجب أن يُظهر رسالة خطأ (Command not found)
  977  ls -la ~/.ollama # يجب أن يُظهر رسالة خطأ (No such file or directory)
  978  history
  979  ollama
  980  ollama list
  981  ollama run llama3.2:1b
  982  ollama serve
  983  ollama list
  984  sudo apt purge google-chrome-canary
  985  sudo apt autoremove
  986  sudo apt purge google-chrome
  987  sudo apt remove google-chrome
  988  sudo apt purge google-chrome-stable
  989  sudo apt autoremove
  990  history
  991  ollama list
  992  open web ui
  993  web-ui server
  994  open-webui serve
  995  ollama 
  996  ollama list
  997  sudo systemctl edit ollama
  998  sudo systemctl edit ollama.service
  999  OLLAMA_ORIGINS=chrome-extension://* ollama serve
 1000  sudo snap remove ollama
 1001  curl -fsSL https://ollama.com/install.sh | sh
 1002  ollama
 1003  ollama serve
 1004  ollama
 1005  ollama run llama3.2:1b
 1006  sudo systemctl edit ollama
 1007  sudo systemctl daemon-reload
 1008  sudo systemctl restart ollama
 1009  ollama list
 1010  ollama
 1011  sudo systemctl daemon-reload
 1012  sudo systemctl restart ollama
 1013  nvidia-smi
 1014  git clone https://github.com/n8n-io/n8n.git
 1015  node ./n8n
 1016  npm run watch
 1017  npx n8n-atom 
 1018  n8n-atom.startServer: 
 1019  n8n-atom.startServer
 1020  run n8n
 1021  npx n8n
 1022  uv venv --python 3.12 venv && source venv/bin/activate
 1023  uv pip install leann
 1024  python
 1025  history
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ pythonn
Command 'pythonn' not found, did you mean:
  command 'python0' from snap python0 (0.9.1)
  command 'python3' from deb python3 (3.12.3-0ubuntu2.1)
See 'snap info <snapname>' for additional versions.
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> from pathlib import Path
>>> from leann.api import LeannBuilder, LeannSearcher, LeannChat
>>> 
>>> # تفعيل السجلات لرؤية التفاصيل
>>> os.environ["LEANN_LOG_LEVEL"] = "INFO" 
>>> 
>>> INDEX_DIR = Path("./").resolve()
>>> INDEX_PATH = str(INDEX_DIR / "demo.leann")
>>> 
>>> # 1. بناء الفهرس (Building Index)
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("C# is a powerful programming language and it is good at game development")
>>> builder.add_text("Python is a powerful programming language and it is good at machine learning tasks")
>>> builder.add_text("Machine learning transforms industries")
>>> builder.add_text("Neural networks process complex data")
>>> builder.add_text("Leann is a great storage saving engine for RAG on your MacBook")
>>> builder.build_index(INDEX_PATH)
INFO - leann.embedding_compute - Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO - leann.embedding_compute - Using device: cuda
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO - leann.embedding_compute - Model loaded successfully! (local + optimized)
INFO - leann.embedding_compute - Applied FP16 precision: facebook/contriever
INFO - leann.embedding_compute - Applied torch.compile optimization: facebook/contriever
INFO - leann.embedding_compute - Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO - leann.embedding_compute - Generated 1 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.3909323215484619 seconds
Writing passages: 100%|███████████████| 5/5 [00:00<00:00, 25922.77chunk/s]
INFO - leann.embedding_compute - Computing embeddings for 5 texts using SentenceTransformer, model: 'facebook/contriever'
INFO - leann.embedding_compute - Using cached optimized model: facebook/contriever
INFO - leann.embedding_compute - Starting embedding computation... (batch_size: 256, manual_tokenize=False)
Batches: 100%|██████████████████████████████| 1/1 [00:00<00:00, 51.04it/s]
INFO - leann.embedding_compute - Generated 5 embeddings, dimension: 768
INFO - leann.embedding_compute - Time taken: 0.023298025131225586 seconds
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (5, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=5
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.18s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=5, Bytes=20
[0.36s]   Read levels (5)
[0.54s]   Probing for compact storage flag...
[0.54s]   Found compact flag: False
[0.54s]   Compact flag is False, reading original format...
[0.54s]   Probing for potential extra byte before non-compact offsets...
[0.54s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=6, Bytes=48
[0.54s]   Read offsets (6)
[0.72s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=320, Bytes=1280
[0.72s]   Read neighbors (320)
[0.89s]   Read scalar params (ep=4, max_lvl=0)
[0.89s] Checking for storage data...
[0.89s]   Found storage fourcc: 49467849.
[0.89s] Converting to CSR format...
[0.90s]   Conversion loop finished.                        
[0.90s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 20
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.90s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=20, |level_ptr|=10
[1.08s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.27s] Conversion complete.
>>> 
>>> # 2. البحث (Searching)
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("programming languages", top_k=2)
2025-11-23 22:55:08,086 - INFO - Starting HNSW server on port 5557 with model facebook/contriever
2025-11-23 22:55:08,086 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:55:08,088 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:55:08,089 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:55:08,089 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:55:08,090 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:55:08,090 - INFO - Started HNSW ZMQ server thread on port 5557
2025-11-23 22:55:08,091 - INFO - HNSW ZMQ REP server listening on port 5557
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.3725090026855469 seconds
2025-11-23 22:55:13,083 - INFO - ⏱️  Text embedding E2E time: 4.992282s
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:55:13,086 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.015662193298339844 seconds
2025-11-23 22:55:13,102 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:55:13,102 - INFO - ⏱️  ZMQ E2E time: 0.019056s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.017651081085205078 seconds
2025-11-23 22:55:13,122 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:55:13,122 - INFO - ⏱️  Distance calculation E2E time: 0.019368s
>>> print(results)
[SearchResult(id='0', score=np.float32(0.98759204), text='C# is a powerful programming language and it is good at game development', metadata={}), SearchResult(id='1', score=np.float32(0.8924463), text='Python is a powerful programming language and it is good at machine learning tasks', metadata={})]
>>> 
>>> # 3. المحادثة (Chatting)
>>> llm_config = {
...     "type": "hf",
...     "model": "Qwen/Qwen2.5-0.5B-Instruct", # الموديل المصحح
... }
>>> 
>>> chat = LeannChat(index_path=INDEX_PATH, llm_config=llm_config)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 5
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 10
[read_HNSW NL v4] Read compact_node_offsets, size: 6
[read_HNSW NL v4] Read entry_point: 4, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 326
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 20
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> 
>>> response = chat.ask(
...     "Compare the two retrieved programming languages and tell me their advantages.",
...     top_k=2,
...     llm_kwargs={"max_tokens": 128},
... )
2025-11-23 22:55:16,860 - INFO - Starting HNSW server on port 5558 with model facebook/contriever
2025-11-23 22:55:16,860 - INFO - Using embedding mode: sentence-transformers
2025-11-23 22:55:16,862 - INFO - Successfully imported unified embedding computation module
2025-11-23 22:55:16,863 - INFO - Loaded PassageManager with 5 passages from metadata
2025-11-23 22:55:16,863 - INFO - Loaded ID map with 5 entries from /home/m/1/demo.ids.txt
2025-11-23 22:55:16,863 - INFO - ZMQ server thread started with shutdown support
2025-11-23 22:55:16,864 - INFO - Started HNSW ZMQ server thread on port 5558
2025-11-23 22:55:16,864 - INFO - HNSW ZMQ REP server listening on port 5558
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Loading and caching optimized SentenceTransformer model: facebook/contriever
INFO:leann.embedding_compute:Using device: cuda
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
INFO:leann.embedding_compute:Model loaded successfully! (local + optimized)
INFO:leann.embedding_compute:Applied FP16 precision: facebook/contriever
INFO:leann.embedding_compute:Applied torch.compile optimization: facebook/contriever
INFO:leann.embedding_compute:Model cached: sentence_transformers_facebook/contriever_cuda_True_optimized
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.3677549362182617 seconds
2025-11-23 22:55:21,839 - INFO - ⏱️  Text embedding E2E time: 4.974213s
ZmqDistanceComputer initialized: d=768, metric=0
2025-11-23 22:55:21,841 - INFO - ZMQ received 1 node IDs for embedding fetch
INFO:leann.embedding_compute:Computing embeddings for 1 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 1 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.017253875732421875 seconds
2025-11-23 22:55:21,859 - INFO - Computed embeddings for 1 texts, shape: (1, 768)
2025-11-23 22:55:21,859 - INFO - ⏱️  ZMQ E2E time: 0.019972s
[HNSW RNG] get_vector_zmq id=4 cache_hit=0
INFO:leann.embedding_compute:Computing embeddings for 4 texts using SentenceTransformer, model: 'facebook/contriever'
INFO:leann.embedding_compute:Using cached optimized model: facebook/contriever
INFO:leann.embedding_compute:Starting embedding computation... (batch_size: 256, manual_tokenize=False)
INFO:leann.embedding_compute:Generated 4 embeddings, dimension: 768
INFO:leann.embedding_compute:Time taken: 0.01971912384033203 seconds
2025-11-23 22:55:21,880 - INFO - Computed embeddings for 4 texts, shape: (4, 768)
2025-11-23 22:55:21,880 - INFO - ⏱️  Distance calculation E2E time: 0.021476s
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.010      | 0          | C# is a powerful programming language and it is good at game |                                                                                 
0.965      | 1          | Python is a powerful programming language and it is good at  |                                                                                 
kwargs in HF:  {'max_tokens': 128}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Certainly! Let's compare C# and Python in terms of their strengths and applications.

### C#
- **Programming Language**: 
  - C# is a statically typed, compiled programming language.
  - It supports multiple development environments such as Visual Studio, MonoDevelop, etc.
  - It has built-in support for object-oriented programming (OOP) and LINQ (Language Integrated Query).
  
- **Applications**:
  - **Game Development**: 
    - Used extensively in games like Unity, Unreal Engine, and Godot.
    - Provides strong performance with optimized compilers.
    - Supports complex game objects, physics engines, and
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
2025-11-23 22:55:40,626 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:55:40,627 - INFO - Initiating graceful shutdown...
2025-11-23 22:55:40,627 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:55:40,903 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:55:40,903 - INFO - ZMQ resources cleaned up
2025-11-23 22:55:41,127 - INFO - Additional resources cleaned up
2025-11-23 22:55:41,127 - INFO - Graceful shutdown completed
2025-11-23 22:55:42,195 - INFO - Received signal 15, shutting down gracefully...
2025-11-23 22:55:42,195 - INFO - Initiating graceful shutdown...
2025-11-23 22:55:42,196 - INFO - Waiting for ZMQ thread to finish...
2025-11-23 22:55:43,157 - INFO - ZMQ server thread exiting gracefully
2025-11-23 22:55:43,157 - INFO - ZMQ resources cleaned up
2025-11-23 22:55:43,389 - INFO - Additional resources cleaned up
2025-11-23 22:55:43,389 - INFO - Graceful shutdown completed
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> from pathlib import Path
>>> 
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> 
>>> # 1. Build an index
>>> builder = LeannBuilder(backend_name="hnsw")
>>> builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
>>> builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")
>>> builder.build_index(INDEX_PATH)
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14979.66chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 61.97it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (2, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/demo.index -> /home/m/1/demo.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=2
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=2, Bytes=8
[0.34s]   Read levels (2)
[0.50s]   Probing for compact storage flag...
[0.50s]   Found compact flag: False
[0.50s]   Compact flag is False, reading original format...
[0.50s]   Probing for potential extra byte before non-compact offsets...
[0.50s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=3, Bytes=24
[0.50s]   Read offsets (3)
[0.67s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=128, Bytes=512
[0.67s]   Read neighbors (128)
[0.84s]   Read scalar params (ep=1, max_lvl=0)
[0.84s] Checking for storage data...
[0.84s]   Found storage fourcc: 49467849.
[0.84s] Converting to CSR format...
[0.84s]   Conversion loop finished.                        
[0.84s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 2
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.84s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=2, |level_ptr|=4
[1.00s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.18s] Conversion complete.
>>> 
>>> # 2. Search
>>> searcher = LeannSearcher(INDEX_PATH)
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> results = searcher.search("fantastical AI-generated creatures", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
>>> # (اختياري) طباعة نتائج البحث
>>> # print(results) 
>>> 
>>> # 3. Chat with your data
>>> chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen2.5-0.5B-Instruct"})
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> 
>>> print(response)
Based on the provided context, LEANN (likely a specific cloud database or storage service) saves 97% of its storage capacity compared to traditional vector databases. This significant savings in terms of storage usage indicates that LEANN offers cost efficiency and scalability advantages over its competitors.
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> from leann import LeannChat
>>> from pathlib import Path
>>> 
>>> # 1. تعريف المسار واسم الموديل
>>> INDEX_PATH = str(Path("./").resolve() / "demo.leann")
>>> model_name = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> # 2. تشغيل الشات
>>> chat = LeannChat(INDEX_PATH, llm_config={
...     "type": "hf", 
...     "model": model_name
... })
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 2
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 4
[read_HNSW NL v4] Read compact_node_offsets, size: 3
[read_HNSW NL v4] Read entry_point: 1, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 242
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 2
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
>>> 
>>> # 3. طرح السؤال وطباعة النتيجة
>>> response = chat.ask("How much storage does LEANN save?", top_k=1)
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=1 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
1.256      | 0          | LEANN saves 97% storage compared to traditional vector datab |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
>>> print(response)
Based on the information provided in the context, LEANN saves 97% of the available storage compared to traditional vector databases. Therefore, the answer to the question "How much storage does LEANN save?" would be 97%.
>>> 








m@m-HP-Z440-Workstation:~/1$ source venv/bin/activate
(venv) m@m-HP-Z440-Workstation:~/1$ uv pip install pypdf==6.4.0
Using Python 3.12.3 environment at: venv
Audited 1 package in 4ms
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ 
(venv) m@m-HP-Z440-Workstation:~/1$ python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> import os
>>> from pathlib import Path
>>> from pypdf import PdfReader
>>> from leann import LeannBuilder, LeannSearcher, LeannChat
>>> 
>>> # --- إعدادات المسارات ---
>>> PDF_PATH = "/home/m/1/Dracula (Novel)_1-5.pdf"  # <--- ضع اسم ملف الـ PDF الخاص بك هنا
>>> INDEX_PATH = str(Path("./").resolve() / "my_book.leann")
>>> MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
>>> 
>>> # --- دالة مساعدة لقراءة وتقسيم ملف الـ PDF ---
>>> def load_and_chunk_pdf(file_path, chunk_size=500, overlap=50):
...     """
...     تقرأ ملف PDF وتقسمه إلى نصوص صغيرة.
...     chunk_size: عدد الحروف في كل فقرة تقريباً.
...     overlap: عدد الحروف المتكررة بين الفقرات لضمان ترابط المعنى.
...     """
...     print(f"📖 جاري قراءة الملف: {file_path}...")
...     reader = PdfReader(file_path)
...     
...     full_text = ""
...     for page in reader.pages:
...         text = page.extract_text()
...         if text:
...             full_text += text + "\n"
...     
...     # عملية التقسيم (Simple Chunking)
...     chunks = []
...     if len(full_text) == 0:
...         print("⚠️ تحذير: لم يتم العثور على نصوص في الملف (قد يكون صوراً).")
...         return []
... 
>>>     for i in range(0, len(full_text), chunk_size - overlap):
  File "<stdin>", line 1
    for i in range(0, len(full_text), chunk_size - overlap):
IndentationError: unexpected indent
>>>         chunks.append(full_text[i:i + chunk_size])
  File "<stdin>", line 1
    chunks.append(full_text[i:i + chunk_size])
IndentationError: unexpected indent
>>>     
>>>     print(f"✅ تم تقسيم الملف إلى {len(chunks)} فقرة (Chunk).")
  File "<stdin>", line 1
    print(f"✅ تم تقسيم الملف إلى {len(chunks)} فقرة (Chunk).")
IndentationError: unexpected indent
>>>     return chunks
  File "<stdin>", line 1
    return chunks
IndentationError: unexpected indent
>>> 
>>> # ==========================================
>>> # 1. بناء الفهرس (Building Index)
>>> # ==========================================
>>> if not os.path.exists(PDF_PATH):
...     print(f"❌ الخطأ: الملف {PDF_PATH} غير موجود في المسار الحالي.")
... else:
...     # استدعاء الدالة لقراءة الملف
...     pdf_chunks = load_and_chunk_pdf(PDF_PATH)
... 
📖 جاري قراءة الملف: /home/m/1/Dracula (Novel)_1-5.pdf...
>>>     if pdf_chunks:
  File "<stdin>", line 1
    if pdf_chunks:
IndentationError: unexpected indent
>>>         print("⚙️ جاري بناء الفهرس وتخزين البيانات...")
  File "<stdin>", line 1
    print("⚙️ جاري بناء الفهرس وتخزين البيانات...")
IndentationError: unexpected indent
>>>         builder = LeannBuilder(backend_name="hnsw")
  File "<stdin>", line 1
    builder = LeannBuilder(backend_name="hnsw")
IndentationError: unexpected indent
>>>         
>>>         # إضافة كل فقرة إلى leann
>>>         for i, chunk in enumerate(pdf_chunks):
  File "<stdin>", line 1
    for i, chunk in enumerate(pdf_chunks):
IndentationError: unexpected indent
>>>             builder.add_text(chunk)
  File "<stdin>", line 1
    builder.add_text(chunk)
IndentationError: unexpected indent
>>>             # طباعة توضيحية كل 100 فقرة
>>>             if (i+1) % 100 == 0: 
  File "<stdin>", line 1
    if (i+1) % 100 == 0: 
IndentationError: unexpected indent
>>>                 print(f"   -> تمت إضافة {i+1} فقرة...")
  File "<stdin>", line 1
    print(f"   -> تمت إضافة {i+1} فقرة...")
IndentationError: unexpected indent
>>> 
>>>         builder.build_index(INDEX_PATH)
  File "<stdin>", line 1
    builder.build_index(INDEX_PATH)
IndentationError: unexpected indent
>>>         print("🎉 تم حفظ قاعدة البيانات بنجاح!")
  File "<stdin>", line 1
    print("🎉 تم حفظ قاعدة البيانات بنجاح!")
IndentationError: unexpected indent
>>> 
>>>         # ==========================================
>>>         # 2. الشات مع الملف (Chatting)
>>>         # ==========================================
>>>         print("\n💬 جاري تشغيل الشات...")
  File "<stdin>", line 1
    print("\n💬 جاري تشغيل الشات...")
IndentationError: unexpected indent
>>>         chat = LeannChat(INDEX_PATH, llm_config={
  File "<stdin>", line 1
    chat = LeannChat(INDEX_PATH, llm_config={
IndentationError: unexpected indent
>>>             "type": "hf", 
  File "<stdin>", line 1
    "type": "hf", 
IndentationError: unexpected indent
>>>             "model": MODEL_NAME
  File "<stdin>", line 1
    "model": MODEL_NAME
IndentationError: unexpected indent
>>>         })
  File "<stdin>", line 1
    })
IndentationError: unexpected indent
>>> 
>>>         # قم بتغيير السؤال هنا بناءً على محتوى كتابك
>>>         question = "Summarize the main topic of this document."
  File "<stdin>", line 1
    question = "Summarize the main topic of this document."
IndentationError: unexpected indent
>>>         print(f"❓ السؤال: {question}")
  File "<stdin>", line 1
    print(f"❓ السؤال: {question}")
IndentationError: unexpected indent
>>>         
>>>         response = chat.ask(question, top_k=3) # top_k=3 لجمع معلومات أكثر من الملف
  File "<stdin>", line 1
    response = chat.ask(question, top_k=3) # top_k=3 لجمع معلومات أكثر من الملف
IndentationError: unexpected indent
>>>         
>>>         print("\n--- الإجابة ---")
  File "<stdin>", line 1
    print("\n--- الإجابة ---")
IndentationError: unexpected indent
>>>         print(response)
  File "<stdin>", line 1
    print(response)
IndentationError: unexpected indent
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> exit()
(venv) m@m-HP-Z440-Workstation:~/1$ python 1.py
📖 جاري قراءة الملف: /home/m/1/Dracula (Novel)_1-5.pdf...
✅ تم تقسيم الملف إلى 22 فقرة (Chunk).
⚙️ جاري بناء الفهرس وتخزين البيانات...
WARNING - sentence_transformers.SentenceTransformer - No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
Writing passages: 100%|███████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 29708.53chunk/s]
Batches: 100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 40.53it/s]
WARNING - leann_backend_hnsw.hnsw_backend - Converting data to float32, shape: (22, 768)
M: 64 for level: 0
Starting conversion: /home/m/1/dracula.index -> /home/m/1/dracula.csr.tmp
[0.00s] Reading Index HNSW header...
[0.00s]   Header read: d=768, ntotal=22
[0.00s] Reading HNSW struct vectors...
  Reading vector (dtype=<class 'numpy.float64'>, fmt='d')... Count=6, Bytes=48
[0.00s]   Read assign_probas (6)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=7, Bytes=28
[0.17s]   Read cum_nneighbor_per_level (7)
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=22, Bytes=88
[0.34s]   Read levels (22)
[0.51s]   Probing for compact storage flag...
[0.51s]   Found compact flag: False
[0.51s]   Compact flag is False, reading original format...
[0.51s]   Probing for potential extra byte before non-compact offsets...
[0.51s]   Found and consumed an unexpected 0x00 byte.
  Reading vector (dtype=<class 'numpy.uint64'>, fmt='Q')... Count=23, Bytes=184
[0.51s]   Read offsets (23)
[0.68s]   Attempting to read neighbors vector...
  Reading vector (dtype=<class 'numpy.int32'>, fmt='i')... Count=1408, Bytes=5632
[0.68s]   Read neighbors (1408)
[0.84s]   Read scalar params (ep=9, max_lvl=0)
[0.84s] Checking for storage data...
[0.84s]   Found storage fourcc: 49467849.
[0.84s] Converting to CSR format...
[0.84s]   Conversion loop finished.                        
[0.84s] Running validation checks...
    Checking total valid neighbor count...
    OK: Total valid neighbors = 462
    Checking final pointer indices...
    OK: Final pointers match data size.
[0.84s] Deleting original neighbors and offsets arrays...
    CSR Stats: |data|=462, |level_ptr|=44
[1.01s] Writing CSR HNSW graph data in FAISS-compatible order...
   Pruning embeddings: Writing NULL storage marker.
[1.19s] Conversion complete.
🎉 تم حفظ قاعدة البيانات بنجاح!

💬 جاري تشغيل الشات...
[read_HNSW - CSR NL v4] Reading metadata & CSR indices (manual offset)...
[read_HNSW NL v4] Read levels vector, size: 22
[read_HNSW NL v4] Reading Compact Storage format indices...
[read_HNSW NL v4] Read compact_level_ptr, size: 44
[read_HNSW NL v4] Read compact_node_offsets, size: 23
[read_HNSW NL v4] Read entry_point: 9, max_level: 0
[read_HNSW NL v4] Read storage fourcc: 0x6c6c756e
[read_HNSW NL v4 FIX] Detected FileIOReader. Neighbors size field offset: 802
[read_HNSW NL v4] Reading neighbors data into memory.
[read_HNSW NL v4] Read neighbors data, size: 462
[read_HNSW NL v4] Finished reading metadata and CSR indices.
INFO: Skipping external storage loading, since is_recompute is true.
❓ السؤال: Who is Count Dracula and what are his powers?
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/contriever
WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/contriever. Creating a new one with mean pooling.
ZmqDistanceComputer initialized: d=768, metric=0
[HNSW RNG] get_vector_zmq id=9 cache_hit=0
The context provided to the LLM is:
Relevance  | Chunk id   | Content                                                      | Source                                                                          
------------------------------------------------------------------------------------------------------------------------------------------------------
0.921      | 0          |  
 
 
 
 
 
 
 
DRACULA  
by  
Bram Stoker  
 
 
1897 editio |                                                                                 
0.882      | 4          | ins; one of the wildest and least known portions 
of Europe. |                                                                                 
0.854      | 17         | ered my questions exactly as if he did.  
He and his wife, t |                                                                                 
kwargs in HF:  {}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)

--- الإجابة ---
Based on the context provided, Count Dracula is a character from Bram Stoker's novel "Dracula" who appears to be a vampire-like figure who has supernatural abilities. Here are some key points about Count Dracula:

1. He is described as having human-like features.
2. He is a vampire-like creature, often depicted as having red eyes, a long neck, and a tail.
3. He possesses dark magic and can communicate with people through their dreams.
4. He has magical powers, such as the ability to turn invisible, heal wounds quickly, and create illusions.
5. While Dracula does possess these powers, he also uses them for evil purposes and often murders those he finds useful.

The context mentions that Count Dracula is "frustrated" by not being able to find specific information about the Castle Dracula, indicating that he may be using his magical powers for sinister purposes rather than seeking personal safety.
(venv) m@m-HP-Z440-Workstation:~/1$ 

